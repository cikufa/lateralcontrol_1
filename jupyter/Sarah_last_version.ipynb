{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cikufa/lateralcontrol_1/blob/main/Sarah_last_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VVh3mWPIJyp",
        "outputId": "99fe33ef-425e-4b97-ca39-1c9ae7049ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.3\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "#saarhin\n",
        "#tasks: 1-action , preview dist in in reward/ 2- one step ahead in network input/ 3- reward every 5 iteration \n",
        "import numpy as np \n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import shapely.geometry as geom\n",
        "from shapely.ops import nearest_points\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import adam_v2\n",
        "import tensorflow_probability as tfp\n",
        "import os\n",
        "from keras.layers import Dense\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "#from keras.optimizers import adam\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbQ0B2DcIOs4"
      },
      "outputs": [],
      "source": [
        "class GenericNetwork(keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims, fc2_dims, name, chkpt_dir=\"/tmp/actor_critic\"):\n",
        "        super(GenericNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.fc3 = Dense(n_actions)\n",
        "        \n",
        "        #self.v = Dense(1, activation=None)\n",
        "        #continous action is represented as a normal distribution that is characterized with 2 quantities: a mean and a standard deviation \n",
        "        #self.pi = Dense(n_actions=2, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e_3NTEAbhqu"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, layer1_dim=128, layer2_dim=64, n_actions=2, alpha_A=0.00003, alpha_C=0.00005, gamma=0.99):\n",
        "        self.layer1_dim = layer1_dim\n",
        "        self.layer2_dim = layer2_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.alpha_A = alpha_A \n",
        "        self.alpha_C= alpha_C\n",
        "        self.action = None\n",
        "        self.log_prob= None\n",
        "        \n",
        "        self.actor = GenericNetwork(n_actions, layer1_dim, layer2_dim, \"actor\")\n",
        "        self.actor.compile(optimizer=adam_v2.Adam(learning_rate=alpha_A))\n",
        "        self.critic = GenericNetwork(1, layer1_dim, layer2_dim, \"critic\")\n",
        "        self.critic.compile(optimizer=adam_v2.Adam(learning_rate=alpha_C))\n",
        "        self.aloss= []\n",
        "        self.closs=[]\n",
        "\n",
        "    def choose_action(self, observation): #obs shape (1,2)\n",
        "        state = tf.convert_to_tensor([observation]) #state shape (1,1,2)\n",
        "        pars= self.actor(state) #mean and standard deviation that make action probs\n",
        "        pars= np.asarray(tf.squeeze(pars)).reshape(1,2)  \n",
        "        sigma , mu = np.hsplit(pars , 2)\n",
        "        sigma = tf.exp(sigma) #get rid of negative sigma\n",
        "        #sigma= abs(sigma)\n",
        "        action_probabilities = tfp.distributions.Normal(mu , sigma) #normal distribution with mu,sigma pars  \n",
        "        #log_prob = action_probabilities.log_prob(action_probabilities) #log (gonna be used for gradient)\n",
        "        action = action_probabilities.sample() #choose action (most likely to be chosen with higher probability)\n",
        "        action = tf.tanh(action) * 0.07 #action: continuous num in range(-0.07, 0.07)((-4,4) degree_\n",
        "        self.action = action  \n",
        "        return action #cast tensor to numpy(openAI gym doesnt take tensor)\n",
        "\n",
        "    # def save_models(self):\n",
        "    #     #print('... saving models ...')\n",
        "    #     self.actor.save_weights(self.actor.checkpoint_file)\n",
        "    #     self.critic.save_weights(self.critic.checkpoint_file)\n",
        "    # def load_models(self):\n",
        "    #     print('... loading models ...')\n",
        "    #     self.actor.load_weights(self.actor.checkpoint_file)\n",
        "    #     self.critic.load_weights(self.critic.checkpoint_file)\n",
        "        \n",
        "    def learn(self, state, reward, state_,done):\n",
        "        #print(\"state before \")\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
        "        reward = tf.convert_to_tensor(reward, dtype=tf.float32) # not fed to NN -> no need to reshape\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            state_value = self.critic(state)\n",
        "            state_value_ = self.critic(state_)\n",
        "            state_value = tf.squeeze(state_value) #squeeze Removes dims of size 1 from the shape of a tensor.\n",
        "            state_value_ = tf.squeeze(state_value_)\n",
        "            pars= self.actor(state)\n",
        "            #pars= np.asarray(tf.squeeze(pars)).reshape(1,2)\n",
        "            #mu , sigma= np.hsplit(pars , 2)\n",
        "            #mu = np.squeeze(mu)\n",
        "            #sigma = np.squeeze(sigma)\n",
        "            mu = pars[0,0]\n",
        "            sigma = pars[0,1]\n",
        "            #print(sigma)\n",
        "            #sigma = tf.exp(sigma)\n",
        "            #print(sigma)\n",
        "            action_probs = tfp.distributions.Normal(mu, abs(sigma)) #policy \n",
        "            log_prob = action_probs.log_prob(self.action[0,0] )\n",
        "            #print(mu,sigma)\n",
        "            #print(log_prob)\n",
        "                      \n",
        "            #TD error: \n",
        "            TD= self.gamma*state_value_*(1-int(done)) - state_value \n",
        "            delta = reward + TD #1-done: terminal stRemoves dimensions of size 1 from the shape of a tensor.ate zero effect \n",
        "            actor_loss = (-log_prob*delta)            \n",
        "            critic_loss = (delta**2) \n",
        "            #print(\"sig\", sigma , \"ac\", actor_loss, \"cr\", critic_loss)\n",
        "  \n",
        "            \n",
        "        gradient1 = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        \n",
        "        self.actor.optimizer.apply_gradients((grad , var) for (grad , var) in zip(gradient1, self.actor.trainable_variables) if grad is not None)\n",
        "        #if grad is not None\n",
        "            \n",
        "        gradient2 = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients((grad , var) for (grad , var) in zip(gradient2, self.critic.trainable_variables) if grad is not None)\n",
        "        # if grad is not None\n",
        "        return critic_loss, actor_loss, gradient1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7jAXNqRskpg"
      },
      "outputs": [],
      "source": [
        "class lateralenv:\n",
        "    def __init__(self, data, data_length, n_episodes, episode_length):\n",
        "        # constants\n",
        "        dt = 0.01\n",
        "        vx = 10\n",
        "        iz = 2278.8\n",
        "        m = 1300\n",
        "        a1 = 1;\n",
        "        a2 = 1.5\n",
        "        caf = 60000\n",
        "        car = 60000\n",
        "        cb = -(caf + car);\n",
        "        cr = (-a1 * caf + a2 * car) / vx\n",
        "        db = -(a1 * caf - a2 * car);\n",
        "        dr = -(a1 ** 2 * caf + a2 ** 2 * car) / vx\n",
        "        cd = caf;\n",
        "        dd = a1 * caf\n",
        "        self.constants = [dt, vx, iz, m, cb, cr, db, dr, cd, dd]\n",
        "\n",
        "        self.data_length = data_length\n",
        "        self.n_episodes = n_episodes\n",
        "        self.episode_length = episode_length\n",
        "        self.episode_length_cnt = episode_length\n",
        "        \n",
        "        self.road = data[0:data_length, :]\n",
        "        self.x = data[0:data_length, 0]\n",
        "        self.y = data[0:data_length, 1]\n",
        "       \n",
        "        self.heading_angle = [np.arctan2(self.y[i + 1] - self.y[i], self.x[i + 1] - self.x[i]) for i in\n",
        "                         range(self.data_length-1)] # rad [-1.57, 1.57]\n",
        "        self.heading_angle.insert(0, self.heading_angle[0])  # append last value to adjust the shape\n",
        "        self.heading_angle = np.asfarray(self.heading_angle).reshape(self.data_length, 1)\n",
        "\n",
        "        # ______________________________________________init vars_____________________________________________________________\n",
        "     \n",
        "        self.score = 0\n",
        "        self.index = 0\n",
        "        self.Done = 0\n",
        "        self.coordinates = []\n",
        "        self.nearestPiontCheck = []\n",
        "        self.vys = []\n",
        "        self.vymax = -10\n",
        "        self.vars = np.zeros((5, 1))\n",
        "        self.vars_ = np.zeros((5, 1), dtype='float64')  # is only updated for normal step\n",
        "        self.vars_tmp= np.zeros((5, 1)) # is updated for both normal step and preview step\n",
        "\n",
        "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def dist_diff(self, ep, limit_dist, limit_ang, stp, pre_point=geom.Point(0,0)):\n",
        "        vy, r, x, y, psi = self.vars_tmp\n",
        "        ##1: based on where the car is supposed to be on this iteration\n",
        "        # self.index = self.index+1\n",
        "        # dist = self.data[self.index, 0:2] - coordinate_ #(1,2)\n",
        "        # angle_diff= self.data[self.index,2] - psi_\n",
        "        # ________________________________________________________________________\n",
        "\n",
        "        ## 2: based on where the car is supposed to be if the driven distance was along the road\n",
        "        # driving_distance= (vy**2 + vx**2)**0.5 *dt #driving angle : psi_\n",
        "        # #(dx^2+dy^2)^0.5= distance , dy=1.5dx -> (3.25dx^2)*0.5 = distance -> 1.802 dx = distance\n",
        "        # dx= driving_distance / math.sqrt(3.25); dy= dx*1.5\n",
        "        # dist= ((dx - x_)**2 + (dy- y_)**2)**0.5\n",
        "        # angle_diff =\n",
        "        # _______________________________________________________________________\n",
        "\n",
        "        # 3: based on car's vertical disance with the road\n",
        "        point = geom.Point(x, y)\n",
        "        dist = point.distance(self.road_ep)\n",
        "        #dist_z = math.sqrt((y - self.y0) ** 2 + (x - self.x0) ** 2)\n",
        "        limited_dist = max(dist, 0.01)\n",
        "        limited_dist = min(limited_dist, 100)\n",
        "\n",
        "        nearestP = nearest_points(self.road_ep, point)[0]\n",
        "        self.nearestPiontCheck.append(np.array(nearestP))\n",
        "        self.nearestPiontCheck.append(np.array(point))\n",
        "        road_slope = (nearestP.y-pre_point.y)/(nearestP.x-pre_point.x) if (nearestP.x-pre_point.x) !=0 else (nearestP.y-pre_point.y)/0.001\n",
        "        angle_diff = abs(np.arctan2((road_slope-psi), 1))[0]\n",
        "        #angle_diff = abs(np.arctan2((nearestP.y-pre_point.y),nearestP.x-pre_point.x)- psi[0]) #sara \n",
        "\n",
        "        #index, = np.where(self.road_ep == nearestP)\n",
        "        # print(\"index\", index)\n",
        "        #angle_diff=  np.arctan2(self.road_ep[index+1][1]-self.road_ep[index][1], self.road_ep[index+1][0]- self.road_ep[index][0]) - psi\n",
        "        # angle_diff = abs((np.cos(x / 100) / 4 - psi)[0])\n",
        "        limited_angle_diff = max(angle_diff, 0.005)\n",
        "        # limited_angle_diff=min(limited_angle_diff , 100) #-> max reward = 5000, min reward 5e-5\n",
        "\n",
        "        # print(\"point\", point,\"nearest p\", nearestP.coords[0], \"angle_diff\", angle_diff)\n",
        "\n",
        "        # debug\n",
        "        # p_buffer=np.zeros((500, 500)) ; dist_buffer=np.zeros((500, 500,3))\n",
        "        # assert p_buffer[ep][stp][0] != point.coords[0][0] , \"equal points !!\"\n",
        "        # assert dist_buffer[ep][stp] != [dist, angle_diff, dist_z] , 'equal dist !!'\n",
        "        # p_buffer[ep][stp]= point.coords[0][0]\n",
        "        # dist_buffer[ep][stp]= [dist, angle_diff, dist_z]\n",
        "\n",
        "        # print(\"point\", point)\n",
        "        # print(\"dist\" , dist, \"angle\", angle_diff, \"Z  \", dist_z)\n",
        "\n",
        "        if limit_dist == 1:\n",
        "            if limit_ang == 1:\n",
        "                return limited_dist, limited_angle_diff, nearestP\n",
        "            elif limit_ang == 0:\n",
        "                return limited_dist, angle_diff, nearestP\n",
        "        else:\n",
        "            return dist, angle_diff, nearestP\n",
        "\n",
        "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def preview(self, action,\n",
        "                preview):  # in this version the preview point is calculated using the updated self.vars. also try with non-updated vars\n",
        "        dt, vx, iz, m, cb, cr, db, dr, cd, dd = self.constants\n",
        "        vy, r, x, y, psi = np.vsplit(self.vars, 5)\n",
        "        if preview == 1:\n",
        "            dt = 0.3\n",
        "        if preview == 0:\n",
        "            dt = 0.1\n",
        "        # calc new state\n",
        "        par_mat1 = np.array([[cb / (m * vx), cr / m - vx, 0, 0, 0],\n",
        "                             [db / (iz * vx), dr / iz, 0, 0, 0],\n",
        "                             [-math.sin(psi), 0, 0, 0, 0],\n",
        "                             [math.cos(psi), 0, 0, 0, 0],\n",
        "                             [0, 1, 0, 0, 0]])\n",
        "\n",
        "        par_mat2 = np.array([[cd * action / m], [dd * action / iz], [vx * math.cos(psi)],\n",
        "                             [vx * math.sin(psi)], [0]], dtype='float64')\n",
        "\n",
        "        var_dot_mat = par_mat1 @ self.vars + par_mat2  # (5,1)= (5,5)@(5,1)+(5,1)\n",
        "        self.vars_tmp = self.vars + dt * var_dot_mat  # (5,1) =(5,1)+(5,1)\n",
        "\n",
        "        if preview == 0:\n",
        "            self.vars_ = self.vars_tmp\n",
        "\n",
        "        return\n",
        "\n",
        "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def step(self, action, stp_cnt, pre_point):\n",
        "        self.preview(action, preview=0)\n",
        "        # print(\"____________________________NOT preview_________________________\")\n",
        "        dist, angle_diff, pre_point = lateralenv.dist_diff(self, ep=0, limit_dist=1, limit_ang=0, stp=stp_cnt, pre_point=pre_point)\n",
        "        # print(\"step :  dist\" , dist, \"angle\", angle_diff, \"Z  \", dist_z)\n",
        "        self.preview(action, preview=1)\n",
        "        # print(\"____________________________preview______________________________\")\n",
        "        future_dist, future_angle_diff, _ = lateralenv.dist_diff(self, ep=0, limit_dist=1, limit_ang=0,\n",
        "                                                                             stp=stp_cnt)\n",
        "        # print(stp_cnt)\n",
        "\n",
        "        # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% calc reward %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "        # 3: based on car's vertical disance with the road\n",
        "        weight = 1\n",
        "        action_weight = -10\n",
        "        preview_weight = 0.01\n",
        "        # print(\"point\", point, dist, \"ang\", angle_diff)\n",
        "        # print(\"dist\", dist, \"ang\", angle_diff)\n",
        "        k1 = 1 / (dist ** 2 + angle_diff ** 2)\n",
        "        k2 = 1 / (future_dist ** 2 + future_angle_diff ** 2)\n",
        "        #reward = weight * k1 + preview_weight * k2  # + action_weight * int(action)\n",
        "        reward_calc = f'{weight} * {k1} + {preview_weight}*{k2} + {action_weight} * {action}'\n",
        "        # reward = - angle_diff\n",
        "\n",
        "        ## 4: Sarah test\n",
        "        # ------------------------\n",
        "        reward = k1 * weight + k2 * preview_weight + action_weight * action \n",
        "       \n",
        "        # print(\"dist\", dist,\"angd\", angle_diff, \"p dist\",future_dist, \"p angd\", future_angle_diff)\n",
        "        self.state_ = np.array([dist, angle_diff, future_dist, future_angle_diff])  # real state (not limited)\n",
        "        # self.state_ = np.array([dist, angle_diff]) #real state (not limited)\n",
        "        \n",
        "        # for next step\n",
        "        self.vars = self.vars_\n",
        "        self.coordinates.append(self.vars[2:4, 0])\n",
        "        self.vys.append(self.vars[0])\n",
        "\n",
        "        self.episode_length_cnt = self.episode_length_cnt - 1\n",
        "        if self.episode_length_cnt == 0:\n",
        "            self.Done = 1\n",
        "\n",
        "        return self.vars_, self.state_, reward, reward_calc, self.Done, pre_point  # state:(dist, ang_dif)\n",
        "\n",
        "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def render(self, ep, score):\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.plot(self.road_ep.coords.xy[0][:], self.road_ep.coords.xy[1][:], 'r') #road\n",
        "        plt.plot(np.array(self.coordinates)[:, 0], np.array(self.coordinates)[:, 1], label=score)  # path\n",
        "\n",
        "        # to check whether the nearest points for a random road works logically\n",
        "        # for i in range(0, len(self.nearestPiontCheck)-10, 100):\n",
        "        #     plt.plot([np.array(self.nearestPiontCheck)[i, 0], np.array(self.nearestPiontCheck)[i+1, 0]],\n",
        "        #                [np.array(self.nearestPiontCheck)[i, 1], np.array(self.nearestPiontCheck)[i + 1, 1]], marker='o')\n",
        "            \n",
        "        plt.legend()\n",
        "        if (ep % 1 == 0):\n",
        "            #plt.show()\n",
        "            plt.savefig(f\"drive/MyDrive/RL_lane_following_debug/path{ep}.jpg\")\n",
        "            plt.cla()\n",
        "        pass\n",
        "\n",
        "    def reset(self, ep):  # before each episode\n",
        "        res = 8\n",
        "        self.Done = 0\n",
        "        self.episode_length_cnt = self.episode_length\n",
        "        self.coordinates = []\n",
        "        self.nearestPiontCheck = []\n",
        "\n",
        "        # a new section of the road excel is selected for each episode\n",
        "        data_ep= self.road[(ep-1)*self.episode_length : (ep-1)*self.episode_length + int(self.episode_length/res), :]\n",
        "        self.road_ep = geom.LineString(zip(self.x[(ep-1)*self.episode_length  : (ep-1)*self.episode_length + int(self.episode_length / res)], \n",
        "                                        self.y[(ep-1)*self.episode_length : (ep-1)*self.episode_length + int(self.episode_length/res)])) #500*2\n",
        "                                    \n",
        "        # the car starts on the road\n",
        "        st_vy = 0; st_r=0;\n",
        "        st_x = data_ep[0,0] + np.random.rand()*3\n",
        "        st_y = data_ep[0,1] + np.random.rand()*3\n",
        "        st_psi = self.heading_angle[(ep-1)*self.episode_length] + np.random.rand()*0.1\n",
        "        st_pre_point = geom.Point(st_x, st_y)\n",
        "        \n",
        "        self.vars = np.array([[st_vy, st_r, st_x, st_y, st_psi]], dtype='float64').T\n",
        "        self.vars_tmp = np.array([[st_vy, st_r, st_x, st_y, st_psi]], dtype='float64').T # is updated for both normal step and preview step\n",
        "        \n",
        "        #point0_ep = geom.Point(st_x, st_y)\n",
        "        limited_dist0, limited_angle_diff0, _ = self.dist_diff(ep=0, limit_dist=1, limit_ang=0, stp=0)\n",
        "        self.preview(action=0, preview=1)\n",
        "        future_limited_dist0, future_limited_ang0, _ = self.dist_diff(ep=0, limit_dist=1, limit_ang=0, stp=0)  # sefr\n",
        "        \n",
        "        state0_ep = np.array([limited_dist0, limited_angle_diff0, limited_dist0, limited_angle_diff0])  # (1,4)\n",
        "    \n",
        "        return state0_ep, st_pre_point"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = 'drive/MyDrive/RL_lane_following_debug/Chaos-generted road.csv'\n",
        "road = pd.read_csv(uploaded).to_numpy()\n",
        "# r= road[0:100, :]\n",
        "# plt.plot(r[:,0], r[:,1])\n",
        "# print(r[0,0])\n",
        "print(road.shape[0])\n",
        "\n",
        "# print(discrete_road_np.shape)\n",
        "# plt.plot(discrete_road_np[:,0], discrete_road_np[:,1])\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "723Vt7gE00R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc5e349-51ba-4d85-ef47-ca39cfc06849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fw_1Aum5snkq",
        "outputId": "eb1adbe5-f444-4a0e-bef0-b12a269eaaa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 :____________________________________________________________________________\n",
            "episode  1 score tf.Tensor([[3023.0427]], shape=(1, 1), dtype=float32) avg_score 3023.0427 reward tf.Tensor([[0.70014924]], shape=(1, 1), dtype=float32)\n",
            "2 :____________________________________________________________________________\n",
            "episode  2 score tf.Tensor([[37.425358]], shape=(1, 1), dtype=float32) avg_score 1530.234 reward tf.Tensor([[-0.699899]], shape=(1, 1), dtype=float32)\n",
            "3 :____________________________________________________________________________\n",
            "episode  3 score tf.Tensor([[117.461426]], shape=(1, 1), dtype=float32) avg_score 1059.3098 reward tf.Tensor([[0.70010096]], shape=(1, 1), dtype=float32)\n",
            "4 :____________________________________________________________________________\n",
            "episode  4 score tf.Tensor([[91.4142]], shape=(1, 1), dtype=float32) avg_score 817.33594 reward tf.Tensor([[0.70010096]], shape=(1, 1), dtype=float32)\n",
            "5 :____________________________________________________________________________\n",
            "episode  5 score tf.Tensor([[-64.48438]], shape=(1, 1), dtype=float32) avg_score 640.97186 reward tf.Tensor([[-0.6998682]], shape=(1, 1), dtype=float32)\n",
            "6 :____________________________________________________________________________\n",
            "episode  6 score tf.Tensor([[-36.95559]], shape=(1, 1), dtype=float32) avg_score 527.98395 reward tf.Tensor([[-0.699899]], shape=(1, 1), dtype=float32)\n",
            "7 :____________________________________________________________________________\n",
            "episode  7 score tf.Tensor([[-4.026033]], shape=(1, 1), dtype=float32) avg_score 451.9825 reward tf.Tensor([[0.6453951]], shape=(1, 1), dtype=float32)\n",
            "8 :____________________________________________________________________________\n",
            "episode  8 score tf.Tensor([[23.550863]], shape=(1, 1), dtype=float32) avg_score 398.4286 reward tf.Tensor([[0.69876343]], shape=(1, 1), dtype=float32)\n",
            "9 :____________________________________________________________________________\n",
            "episode  9 score tf.Tensor([[58.637836]], shape=(1, 1), dtype=float32) avg_score 360.67407 reward tf.Tensor([[-0.6327774]], shape=(1, 1), dtype=float32)\n",
            "10 :____________________________________________________________________________\n",
            "episode  10 score tf.Tensor([[23.598753]], shape=(1, 1), dtype=float32) avg_score 326.96652 reward tf.Tensor([[0.7002063]], shape=(1, 1), dtype=float32)\n",
            "11 :____________________________________________________________________________\n",
            "episode  11 score tf.Tensor([[62.546722]], shape=(1, 1), dtype=float32) avg_score 302.92834 reward tf.Tensor([[0.7002678]], shape=(1, 1), dtype=float32)\n",
            "12 :____________________________________________________________________________\n",
            "episode  12 score tf.Tensor([[81.62324]], shape=(1, 1), dtype=float32) avg_score 284.48627 reward tf.Tensor([[0.7007346]], shape=(1, 1), dtype=float32)\n",
            "13 :____________________________________________________________________________\n",
            "episode  13 score tf.Tensor([[47.774715]], shape=(1, 1), dtype=float32) avg_score 266.27768 reward tf.Tensor([[0.70012486]], shape=(1, 1), dtype=float32)\n",
            "14 :____________________________________________________________________________\n",
            "episode  14 score tf.Tensor([[113.595695]], shape=(1, 1), dtype=float32) avg_score 255.37183 reward tf.Tensor([[-0.69973195]], shape=(1, 1), dtype=float32)\n",
            "15 :____________________________________________________________________________\n",
            "episode  15 score tf.Tensor([[33.15362]], shape=(1, 1), dtype=float32) avg_score 240.55728 reward tf.Tensor([[-0.69989336]], shape=(1, 1), dtype=float32)\n",
            "16 :____________________________________________________________________________\n",
            "episode  16 score tf.Tensor([[127.37567]], shape=(1, 1), dtype=float32) avg_score 233.48344 reward tf.Tensor([[0.70010096]], shape=(1, 1), dtype=float32)\n",
            "17 :____________________________________________________________________________\n",
            "episode  17 score tf.Tensor([[1770.7233]], shape=(1, 1), dtype=float32) avg_score 323.90933 reward tf.Tensor([[0.7005402]], shape=(1, 1), dtype=float32)\n",
            "18 :____________________________________________________________________________\n",
            "episode  18 score tf.Tensor([[790.2537]], shape=(1, 1), dtype=float32) avg_score 349.81735 reward tf.Tensor([[0.70174384]], shape=(1, 1), dtype=float32)\n",
            "19 :____________________________________________________________________________\n",
            "episode  19 score tf.Tensor([[929.34564]], shape=(1, 1), dtype=float32) avg_score 380.31885 reward tf.Tensor([[0.70028925]], shape=(1, 1), dtype=float32)\n",
            "20 :____________________________________________________________________________\n",
            "episode  20 score tf.Tensor([[949.7277]], shape=(1, 1), dtype=float32) avg_score 408.78928 reward tf.Tensor([[0.70080423]], shape=(1, 1), dtype=float32)\n",
            "21 :____________________________________________________________________________\n",
            "episode  21 score tf.Tensor([[60.204937]], shape=(1, 1), dtype=float32) avg_score 392.19 reward tf.Tensor([[0.7017078]], shape=(1, 1), dtype=float32)\n",
            "22 :____________________________________________________________________________\n",
            "episode  22 score tf.Tensor([[276.07434]], shape=(1, 1), dtype=float32) avg_score 386.91202 reward tf.Tensor([[0.70016974]], shape=(1, 1), dtype=float32)\n",
            "23 :____________________________________________________________________________\n",
            "episode  23 score tf.Tensor([[-5.1782417]], shape=(1, 1), dtype=float32) avg_score 369.8646 reward tf.Tensor([[0.7001144]], shape=(1, 1), dtype=float32)\n",
            "24 :____________________________________________________________________________\n",
            "episode  24 score tf.Tensor([[1052.663]], shape=(1, 1), dtype=float32) avg_score 398.31454 reward tf.Tensor([[-0.66162586]], shape=(1, 1), dtype=float32)\n",
            "25 :____________________________________________________________________________\n",
            "episode  25 score tf.Tensor([[-29.962725]], shape=(1, 1), dtype=float32) avg_score 381.18344 reward tf.Tensor([[0.70010096]], shape=(1, 1), dtype=float32)\n",
            "26 :____________________________________________________________________________\n",
            "episode  26 score tf.Tensor([[-31.285303]], shape=(1, 1), dtype=float32) avg_score 365.31927 reward tf.Tensor([[-0.6998869]], shape=(1, 1), dtype=float32)\n",
            "27 :____________________________________________________________________________\n",
            "episode  27 score tf.Tensor([[386.3209]], shape=(1, 1), dtype=float32) avg_score 366.0971 reward tf.Tensor([[0.70010096]], shape=(1, 1), dtype=float32)\n",
            "28 :____________________________________________________________________________\n",
            "episode  28 score tf.Tensor([[-18.641197]], shape=(1, 1), dtype=float32) avg_score 352.35645 reward tf.Tensor([[0.70010096]], shape=(1, 1), dtype=float32)\n",
            "29 :____________________________________________________________________________\n",
            "episode  29 score tf.Tensor([[-99.0186]], shape=(1, 1), dtype=float32) avg_score 336.79178 reward tf.Tensor([[-0.699899]], shape=(1, 1), dtype=float32)\n",
            "30 :____________________________________________________________________________\n",
            "episode  30 score tf.Tensor([[227.56157]], shape=(1, 1), dtype=float32) avg_score 333.1508 reward tf.Tensor([[-0.699899]], shape=(1, 1), dtype=float32)\n",
            "31 :____________________________________________________________________________\n",
            "episode  31 score tf.Tensor([[-58.69261]], shape=(1, 1), dtype=float32) avg_score 320.51068 reward tf.Tensor([[0.70010096]], shape=(1, 1), dtype=float32)\n",
            "32 :____________________________________________________________________________\n",
            "episode  32 score tf.Tensor([[-105.750595]], shape=(1, 1), dtype=float32) avg_score 307.19003 reward tf.Tensor([[-0.69984114]], shape=(1, 1), dtype=float32)\n",
            "33 :____________________________________________________________________________\n",
            "episode  33 score tf.Tensor([[-34.019863]], shape=(1, 1), dtype=float32) avg_score 296.85034 reward tf.Tensor([[-0.699899]], shape=(1, 1), dtype=float32)\n",
            "34 :____________________________________________________________________________\n",
            "episode  34 score tf.Tensor([[-119.06544]], shape=(1, 1), dtype=float32) avg_score 284.61752 reward tf.Tensor([[-0.6997445]], shape=(1, 1), dtype=float32)\n",
            "35 :____________________________________________________________________________\n",
            "episode  35 score tf.Tensor([[289.21124]], shape=(1, 1), dtype=float32) avg_score 284.74878 reward tf.Tensor([[0.70010096]], shape=(1, 1), dtype=float32)\n",
            "36 :____________________________________________________________________________\n",
            "episode  36 score tf.Tensor([[578.5517]], shape=(1, 1), dtype=float32) avg_score 292.90997 reward tf.Tensor([[0.7002104]], shape=(1, 1), dtype=float32)\n",
            "37 :____________________________________________________________________________\n",
            "episode  37 score tf.Tensor([[187.67583]], shape=(1, 1), dtype=float32) avg_score 290.0658 reward tf.Tensor([[0.7002856]], shape=(1, 1), dtype=float32)\n",
            "38 :____________________________________________________________________________\n",
            "episode  38 score tf.Tensor([[201.90176]], shape=(1, 1), dtype=float32) avg_score 287.7457 reward tf.Tensor([[0.7002931]], shape=(1, 1), dtype=float32)\n",
            "39 :____________________________________________________________________________\n",
            "episode  39 score tf.Tensor([[170.16467]], shape=(1, 1), dtype=float32) avg_score 284.7308 reward tf.Tensor([[0.7003158]], shape=(1, 1), dtype=float32)\n",
            "40 :____________________________________________________________________________\n",
            "episode  40 score tf.Tensor([[190.07986]], shape=(1, 1), dtype=float32) avg_score 282.3645 reward tf.Tensor([[0.7002993]], shape=(1, 1), dtype=float32)\n",
            "41 :____________________________________________________________________________\n",
            "episode  41 score tf.Tensor([[703.0614]], shape=(1, 1), dtype=float32) avg_score 292.6254 reward tf.Tensor([[0.7002232]], shape=(1, 1), dtype=float32)\n",
            "42 :____________________________________________________________________________\n",
            "episode  42 score tf.Tensor([[189.91464]], shape=(1, 1), dtype=float32) avg_score 290.17993 reward tf.Tensor([[0.70029604]], shape=(1, 1), dtype=float32)\n",
            "43 :____________________________________________________________________________\n",
            "episode  43 score tf.Tensor([[231.48648]], shape=(1, 1), dtype=float32) avg_score 288.81494 reward tf.Tensor([[0.70028406]], shape=(1, 1), dtype=float32)\n",
            "44 :____________________________________________________________________________\n",
            "episode  44 score tf.Tensor([[143.95894]], shape=(1, 1), dtype=float32) avg_score 285.52277 reward tf.Tensor([[0.7003067]], shape=(1, 1), dtype=float32)\n",
            "45 :____________________________________________________________________________\n",
            "episode  45 score tf.Tensor([[1567.6438]], shape=(1, 1), dtype=float32) avg_score 314.01434 reward tf.Tensor([[0.70021284]], shape=(1, 1), dtype=float32)\n",
            "46 :____________________________________________________________________________\n",
            "episode  46 score tf.Tensor([[190.67392]], shape=(1, 1), dtype=float32) avg_score 311.33304 reward tf.Tensor([[0.7002384]], shape=(1, 1), dtype=float32)\n",
            "47 :____________________________________________________________________________\n",
            "episode  47 score tf.Tensor([[7165.919]], shape=(1, 1), dtype=float32) avg_score 457.1753 reward tf.Tensor([[0.7001429]], shape=(1, 1), dtype=float32)\n",
            "48 :____________________________________________________________________________\n",
            "episode  48 score tf.Tensor([[487.14856]], shape=(1, 1), dtype=float32) avg_score 457.7997 reward tf.Tensor([[0.7001989]], shape=(1, 1), dtype=float32)\n",
            "49 :____________________________________________________________________________\n",
            "episode  49 score tf.Tensor([[759.77325]], shape=(1, 1), dtype=float32) avg_score 463.96246 reward tf.Tensor([[0.7002263]], shape=(1, 1), dtype=float32)\n",
            "50 :____________________________________________________________________________\n",
            "episode  50 score tf.Tensor([[436.61038]], shape=(1, 1), dtype=float32) avg_score 463.41544 reward tf.Tensor([[0.7002605]], shape=(1, 1), dtype=float32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "agent = Agent(layer1_dim=128, layer2_dim=64, n_actions=2, alpha_A=0.0003, alpha_C=0.005, gamma=0.99)\n",
        "n_episodes = 50\n",
        "data_length = int(road.shape[0]/10) #10,000\n",
        "ep_length = int(data_length / n_episodes)  #200\n",
        "env = lateralenv(road, data_length, n_episodes, ep_length)\n",
        "\n",
        "score_history = []\n",
        "best_score = 0  # reward = 1/positive > 0 -> min score =0\n",
        "load_checkpoint = False\n",
        "\n",
        "workbook = xlsxwriter.Workbook('drive/MyDrive/RL_lane_following_debug/log.xlsx')\n",
        "log = workbook.add_worksheet(\"ep_per_ep\")\n",
        "log.write(0, 0, \"ep / step\")\n",
        "log.write(0, 3, \"vy\")\n",
        "log.write(0, 4, \"point\")\n",
        "log.write(0, 5, \"distance\")\n",
        "log.write(0, 6, \"angle_diff\")\n",
        "log.write(0, 7, \"road derivative\")\n",
        "log.write(0, 8, \"psi\")\n",
        "log.write(0, 9, \"reward\")\n",
        "log.write(0, 10, \"point dist_diff +  preview point dist_diff + action\")\n",
        "\n",
        "# training________________________________________________________________________________________\n",
        "cnt = 0\n",
        "for ep in range(1, n_episodes + 1):\n",
        "    score = 0\n",
        "    al = [];\n",
        "    cl = [];\n",
        "    rewards = []\n",
        "    state, pre_point = env.reset(ep)  # (1,2)\n",
        "\n",
        "    j = 0\n",
        "    states_ = []\n",
        "    act_buffer = 0\n",
        "    print(ep, \":____________________________________________________________________________\")\n",
        "    while not env.Done:\n",
        "        action = agent.choose_action(state)\n",
        "        # assert action != act_buffer , \"equal actions !!\"\n",
        "        #act_buffer = action\n",
        "\n",
        "        newvars, state_, reward, reward_calc, Done, pre_point = env.step(action, j, pre_point)\n",
        "        states_.append(state_)\n",
        "\n",
        "        score = score + reward\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # if not load_checkpoint:\n",
        "        closs, aloss, grad1 = agent.learn(state, reward, state_, Done)\n",
        "        # log\n",
        "        log.write((ep - 1) * ep_length + j + 1, 0, f\"{ep} / {j}\")\n",
        "        log.write((ep - 1) * ep_length + j + 1, 3, newvars[0])\n",
        "        log.write((ep - 1) * ep_length + j + 1, 4, str(newvars[2:4]))\n",
        "        log.write((ep - 1) * ep_length + j + 1, 5, state_[0])\n",
        "        log.write((ep - 1) * ep_length + j + 1, 6, state_[1])\n",
        "        log.write((ep - 1) * ep_length + j + 1, 7, np.cos(newvars[2] / 200)[0] / 4)\n",
        "        log.write((ep - 1) * ep_length + j + 1, 8, newvars[-1])\n",
        "        log.write((ep - 1) * ep_length + j + 1, 9, reward)\n",
        "        log.write((ep - 1) * ep_length + j + 1, 10, reward_calc)\n",
        "\n",
        "        state = state_\n",
        "        j += 1  # step counter\n",
        "\n",
        "    states_ = np.array(states_)\n",
        "    score_history.append(score)\n",
        "\n",
        "    # ______________ plot score curve)\n",
        "    # ep = [i+1 for i in range(i)]\n",
        "    # x= np.array(ep).reshape(i,1)\n",
        "    # #score_history= np.array(score_history).reshape(i,1)\n",
        "    # plt.xlabel(\"episode\")\n",
        "    # plt.ylabel(\"score\")\n",
        "    # plt.plot(x, np.array(score_history).reshape(i,1))\n",
        "    # plt.savefig(figure_file)\n",
        "\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "    if (ep % 1 == 0):\n",
        "        print('episode ', ep, 'score', score, 'avg_score', avg_score, 'reward', reward)\n",
        "        env.render(ep, score)\n",
        "\n",
        "workbook.close()\n",
        "\n",
        "if not load_checkpoint:\n",
        "    ep = [i + 1 for i in range(n_episodes)]\n",
        "    x = np.array(ep).reshape(n_episodes, 1)\n",
        "    score_history = np.array(score_history).reshape(n_episodes, 1)\n",
        "    plt.xlabel(\"episode\")\n",
        "    plt.ylabel(\"score\")\n",
        "    plt.plot(x, score_history)\n",
        "    plt.savefig('scores.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd5X-6pIQ8vU"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmPtNVj3Px5c"
      },
      "outputs": [],
      "source": [
        "# test \n",
        "import numpy as np \n",
        "import math\n",
        "import shapely.geometry as geom\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.ops import nearest_points\n",
        "\n",
        "# x= np.arange(0, 10).reshape(10, 1) \n",
        "# y= 50*np.sin(x/200)\n",
        "# road = geom.LineString(zip(x,y))\n",
        "# p= geom.Point(314,50)\n",
        "# print(\"ggg\", p.coords[0][1])\n",
        "# print(\"ggg\", p.coords[0][0])\n",
        "# dist = p.distance(road) \n",
        "# print(dist)\n",
        "# nearestP = nearest_points(road, p)\n",
        "# print(nearestP[0])\n",
        "#angle_diff= np.arctan2(nearestP.centroid.y, nearestP.centroid.x) - self.psi0 #pos/neg mide  \n",
        "\n",
        "n=1500\n",
        "x= np.arange(0, n).reshape(n, 1) \n",
        "y= np.sqrt(1-x**2)\n",
        "road = geom.LineString(zip(x,y))\n",
        "plt.plot(x,y)\n",
        " #self.road = genfromtxt(roadfile, delimiter=',')\n",
        "heading_angle = [np.arctan2(y[i+1]-y[i] , x[i+1]-x[i]) for i in range(y.shape[0]-1)] #rad #56.3\n",
        "heading_angle= np.asfarray(heading_angle).reshape(n-1,1)\n",
        "print(max(heading_angle))\n",
        "print(min(heading_angle))\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlAwco_gsuPK"
      },
      "outputs": [],
      "source": [
        "class t:\n",
        "    def __init__(self, roadfile, data_length , n_episodes, episode_length):\n",
        "        #constants\n",
        "        dt=1 #0.1\n",
        "        vx=10\n",
        "        iz= 2278.8\n",
        "        m=1300\n",
        "        a1=1; a2=1.5\n",
        "        caf = 60000; car= 60000\n",
        "        cb= -(caf + car); cr= (-a1*caf + a2*car)/ vx\n",
        "        db= -(a1* caf - a2*car); dr= -(a1**2 *caf + a2**2*car) / vx\n",
        "        cd = caf; dd= a1*caf\n",
        "        self.constants=[dt, vx, iz, m, cb, cr, db, dr, cd, dd]\n",
        "         \n",
        "    def preview(self, point):\n",
        "        action = 0\n",
        "        dt, vx, iz, m, cb, cr, db, dr, cd, dd= self.constants\n",
        "        vy, r, x, y, psi = np.vsplit(self.vars,5)\n",
        "        #dt = dt*5 # 5 step forward preview \n",
        "        #calc new state\n",
        "        par_mat1 = np.array([[cb/(m*vx), cr/m-vx,0,0,0],\n",
        "                           [db/(iz*vx), dr/iz, 0,0,0],\n",
        "                           [-math.sin(psi),0,0,0,0],\n",
        "                           [math.cos(psi),0,0,0,0],\n",
        "                           [0,1,0,0,0]])\n",
        "        \n",
        "        par_mat2 = np.array([[cd* action /m],[dd*action/iz], [vx*math.cos(psi)],\n",
        "                    [vx*math.sin(psi)],[0]], dtype='float64') \n",
        "     \n",
        "        var_dot_mat = par_mat1 @ self.vars + par_mat2  #(5,1)= (5,5)@(5,1)+(5,1)\n",
        "        self.vars_= self.vars + dt* var_dot_mat #(5,1) =(5,1)+(5,1)\n",
        "        vy_, r_, x_, y_, psi_= np.vsplit(self.vars_,5)\n",
        "        future_point= geom.Point(x_, y_)\n",
        "\n",
        "        return future_point\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b5wsD1rXOYR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sarah last version.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}