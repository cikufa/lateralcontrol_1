{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "terminate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6EpTqCbZajDBHHOY7VN6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cikufa/lateralcontrol_1/blob/main/terminate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pm5Vog9rQ3f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VVh3mWPIJyp",
        "outputId": "2ac65f2a-86da-4d06-b55d-cd8fcde6fdb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.7/dist-packages (3.0.3)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "#saarhin\n",
        "#tasks: 1-action , preview dist in in reward/ 2- one step ahead in network input/ 3- reward every 5 iteration \n",
        "import numpy as np \n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import shapely.geometry as geom\n",
        "from shapely.ops import nearest_points\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import adam_v2\n",
        "import tensorflow_probability as tfp\n",
        "import os\n",
        "from keras.layers import Dense\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "#from keras.optimizers import adam\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hbQ0B2DcIOs4"
      },
      "outputs": [],
      "source": [
        "class GenericNetwork(keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims, fc2_dims, name, chkpt_dir=\"/tmp/actor_critic\"):\n",
        "        super(GenericNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.fc3 = Dense(n_actions)\n",
        "        \n",
        "        #self.v = Dense(1, activation=None)\n",
        "        #continous action is represented as a normal distribution that is characterized with 2 quantities: a mean and a standard deviation \n",
        "        #self.pi = Dense(n_actions=2, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "2e_3NTEAbhqu"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, layer1_dim=128, layer2_dim=64, n_actions=2, alpha_A=0.00003, alpha_C=0.00005, gamma=0.99):\n",
        "        self.layer1_dim = layer1_dim\n",
        "        self.layer2_dim = layer2_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.alpha_A = alpha_A \n",
        "        self.alpha_C= alpha_C\n",
        "        self.action = None\n",
        "        self.log_prob= None\n",
        "        \n",
        "        self.actor = GenericNetwork(n_actions, layer1_dim, layer2_dim, \"actor\")\n",
        "        self.actor.compile(optimizer=adam_v2.Adam(learning_rate=alpha_A))\n",
        "        self.critic = GenericNetwork(1, layer1_dim, layer2_dim, \"critic\")\n",
        "        self.critic.compile(optimizer=adam_v2.Adam(learning_rate=alpha_C))\n",
        "        self.aloss= []\n",
        "        self.closs=[]\n",
        "\n",
        "    def choose_action(self, observation): #obs shape (1,2)\n",
        "        state = tf.convert_to_tensor([observation]) #state shape (1,1,2)\n",
        "        pars= self.actor(state) #mean and standard deviation that make action probs\n",
        "        pars= np.asarray(tf.squeeze(pars)).reshape(1,2)  \n",
        "        sigma , mu = np.hsplit(pars , 2)\n",
        "        sigma = tf.exp(sigma) #get rid of negative sigma\n",
        "        #sigma= abs(sigma)\n",
        "        action_probabilities = tfp.distributions.Normal(mu , sigma) #normal distribution with mu,sigma pars  \n",
        "        #log_prob = action_probabilities.log_prob(action_probabilities) #log (gonna be used for gradient)\n",
        "        action = action_probabilities.sample() #choose action (most likely to be chosen with higher probability)\n",
        "        action = tf.tanh(action) * 0.07 #action: continuous num in range(-0.07, 0.07)((-4,4) degree_\n",
        "        self.action = action  \n",
        "        return action #cast tensor to numpy(openAI gym doesnt take tensor)\n",
        "\n",
        "    # def save_models(self):\n",
        "    #     #print('... saving models ...')\n",
        "    #     self.actor.save_weights(self.actor.checkpoint_file)\n",
        "    #     self.critic.save_weights(self.critic.checkpoint_file)\n",
        "    # def load_models(self):\n",
        "    #     print('... loading models ...')\n",
        "    #     self.actor.load_weights(self.actor.checkpoint_file)\n",
        "    #     self.critic.load_weights(self.critic.checkpoint_file)\n",
        "        \n",
        "    def learn(self, state, reward, state_,done):\n",
        "        #print(\"state before \")\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
        "        reward = tf.convert_to_tensor(reward, dtype=tf.float32) # not fed to NN -> no need to reshape\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            state_value = self.critic(state)\n",
        "            state_value_ = self.critic(state_)\n",
        "            state_value = tf.squeeze(state_value) #squeeze Removes dims of size 1 from the shape of a tensor.\n",
        "            state_value_ = tf.squeeze(state_value_)\n",
        "            pars= self.actor(state)\n",
        "            #pars= np.asarray(tf.squeeze(pars)).reshape(1,2)\n",
        "            #mu , sigma= np.hsplit(pars , 2)\n",
        "            #mu = np.squeeze(mu)\n",
        "            #sigma = np.squeeze(sigma)\n",
        "            mu = pars[0,0]\n",
        "            sigma = pars[0,1]\n",
        "            #print(sigma)\n",
        "            #sigma = tf.exp(sigma)\n",
        "            #print(sigma)\n",
        "            action_probs = tfp.distributions.Normal(mu, abs(sigma)) #policy \n",
        "            log_prob = action_probs.log_prob(self.action[0,0] )\n",
        "            #print(mu,sigma)\n",
        "            #print(log_prob)\n",
        "                      \n",
        "            #TD error: \n",
        "            TD= self.gamma*state_value_*(1-int(done)) - state_value \n",
        "            delta = reward + TD #1-done: terminal stRemoves dimensions of size 1 from the shape of a tensor.ate zero effect \n",
        "            actor_loss = (-log_prob*delta)            \n",
        "            critic_loss = (delta**2) \n",
        "            #print(\"sig\", sigma , \"ac\", actor_loss, \"cr\", critic_loss)\n",
        "  \n",
        "            \n",
        "        gradient1 = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        \n",
        "        self.actor.optimizer.apply_gradients((grad , var) for (grad , var) in zip(gradient1, self.actor.trainable_variables) if grad is not None)\n",
        "        #if grad is not None\n",
        "            \n",
        "        gradient2 = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients((grad , var) for (grad , var) in zip(gradient2, self.critic.trainable_variables) if grad is not None)\n",
        "        # if grad is not None\n",
        "        return critic_loss, actor_loss, gradient1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "-7jAXNqRskpg"
      },
      "outputs": [],
      "source": [
        "class lateralenv:\n",
        "    def __init__(self, data, data_length, n_episodes, episode_length):\n",
        "        # constants\n",
        "        dt = 0.01\n",
        "        vx = 10\n",
        "        iz = 2278.8\n",
        "        m = 1300\n",
        "        a1 = 1;\n",
        "        a2 = 1.5\n",
        "        caf = 60000\n",
        "        car = 60000\n",
        "        cb = -(caf + car);\n",
        "        cr = (-a1 * caf + a2 * car) / vx\n",
        "        db = -(a1 * caf - a2 * car);\n",
        "        dr = -(a1 ** 2 * caf + a2 ** 2 * car) / vx\n",
        "        cd = caf;\n",
        "        dd = a1 * caf\n",
        "        self.constants = [dt, vx, iz, m, cb, cr, db, dr, cd, dd]\n",
        "\n",
        "        self.data_length = data_length\n",
        "        self.n_episodes = n_episodes\n",
        "        self.episode_length = episode_length\n",
        "        self.episode_length_cnt = episode_length\n",
        "        \n",
        "        self.road = data[0:data_length, :]\n",
        "        self.x = data[0:data_length, 0]\n",
        "        self.y = data[0:data_length, 1]\n",
        "       \n",
        "        self.heading_angle = [np.arctan2(self.y[i + 1] - self.y[i], self.x[i + 1] - self.x[i]) for i in\n",
        "                         range(self.data_length-1)] # rad [-1.57, 1.57]\n",
        "        self.heading_angle.insert(0, self.heading_angle[0])  # append last value to adjust the shape\n",
        "        self.heading_angle = np.asfarray(self.heading_angle).reshape(self.data_length, 1)\n",
        "\n",
        "        # ______________________________________________init vars_____________________________________________________________\n",
        "     \n",
        "        self.score = 0\n",
        "        self.index = 0\n",
        "        self.Done = 0\n",
        "        self.coordinates = []\n",
        "        self.nearestPiontCheck = []\n",
        "        self.vys = []\n",
        "        self.vymax = -10\n",
        "        self.vars = np.zeros((5, 1))\n",
        "        self.vars_ = np.zeros((5, 1), dtype='float64')  # is only updated for normal step\n",
        "        self.vars_tmp= np.zeros((5, 1)) # is updated for both normal step and preview step\n",
        "\n",
        "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def dist_diff(self, ep, limit_dist, limit_ang, stp, pre_point=geom.Point(0,0)):\n",
        "        vy, r, x, y, psi = self.vars_tmp\n",
        "        ##1: based on where the car is supposed to be on this iteration\n",
        "        # self.index = self.index+1\n",
        "        # dist = self.data[self.index, 0:2] - coordinate_ #(1,2)\n",
        "        # angle_diff= self.data[self.index,2] - psi_\n",
        "        # ________________________________________________________________________\n",
        "\n",
        "        ## 2: based on where the car is supposed to be if the driven distance was along the road\n",
        "        # driving_distance= (vy**2 + vx**2)**0.5 *dt #driving angle : psi_\n",
        "        # #(dx^2+dy^2)^0.5= distance , dy=1.5dx -> (3.25dx^2)*0.5 = distance -> 1.802 dx = distance\n",
        "        # dx= driving_distance / math.sqrt(3.25); dy= dx*1.5\n",
        "        # dist= ((dx - x_)**2 + (dy- y_)**2)**0.5\n",
        "        # angle_diff =\n",
        "        # _______________________________________________________________________\n",
        "\n",
        "        # 3: based on car's vertical disance with the road\n",
        "        point = geom.Point(x, y)\n",
        "        dist = point.distance(self.road_ep)\n",
        "        #dist_z = math.sqrt((y - self.y0) ** 2 + (x - self.x0) ** 2)\n",
        "        limited_dist = max(dist, 0.01)\n",
        "        limited_dist = min(limited_dist, 100)\n",
        "\n",
        "        nearestP = nearest_points(self.road_ep, point)[0]\n",
        "        self.nearestPiontCheck.append(np.array(nearestP))\n",
        "        self.nearestPiontCheck.append(np.array(point))\n",
        "        road_slope = (nearestP.y-pre_point.y)/(nearestP.x-pre_point.x) if (nearestP.x-pre_point.x) !=0 else (nearestP.y-pre_point.y)/0.001\n",
        "        angle_diff = abs(np.arctan2((road_slope-psi), 1))[0]\n",
        "        #angle_diff = abs(np.arctan2((nearestP.y-pre_point.y),nearestP.x-pre_point.x)- psi[0]) #sara \n",
        "\n",
        "        #index, = np.where(self.road_ep == nearestP)\n",
        "        # print(\"index\", index)\n",
        "        #angle_diff=  np.arctan2(self.road_ep[index+1][1]-self.road_ep[index][1], self.road_ep[index+1][0]- self.road_ep[index][0]) - psi\n",
        "        # angle_diff = abs((np.cos(x / 100) / 4 - psi)[0])\n",
        "        limited_angle_diff = max(angle_diff, 0.005)\n",
        "        # limited_angle_diff=min(limited_angle_diff , 100) #-> max reward = 5000, min reward 5e-5\n",
        "\n",
        "        # print(\"point\", point,\"nearest p\", nearestP.coords[0], \"angle_diff\", angle_diff)\n",
        "\n",
        "        # debug\n",
        "        # p_buffer=np.zeros((500, 500)) ; dist_buffer=np.zeros((500, 500,3))\n",
        "        # assert p_buffer[ep][stp][0] != point.coords[0][0] , \"equal points !!\"\n",
        "        # assert dist_buffer[ep][stp] != [dist, angle_diff, dist_z] , 'equal dist !!'\n",
        "        # p_buffer[ep][stp]= point.coords[0][0]\n",
        "        # dist_buffer[ep][stp]= [dist, angle_diff, dist_z]\n",
        "\n",
        "        # print(\"point\", point)\n",
        "        # print(\"dist\" , dist, \"angle\", angle_diff, \"Z  \", dist_z)\n",
        "\n",
        "        if limit_dist == 1:\n",
        "            if limit_ang == 1:\n",
        "                return limited_dist, limited_angle_diff, nearestP\n",
        "            elif limit_ang == 0:\n",
        "                return limited_dist, angle_diff, nearestP\n",
        "        else:\n",
        "            return dist, angle_diff, nearestP\n",
        "\n",
        "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def preview(self, action,\n",
        "                preview):  # in this version the preview point is calculated using the updated self.vars. also try with non-updated vars\n",
        "        dt, vx, iz, m, cb, cr, db, dr, cd, dd = self.constants\n",
        "        vy, r, x, y, psi = np.vsplit(self.vars, 5)\n",
        "        if preview == 1:\n",
        "            dt = 0.3\n",
        "        if preview == 0:\n",
        "            dt = 0.1\n",
        "        # calc new state\n",
        "        par_mat1 = np.array([[cb / (m * vx), cr / m - vx, 0, 0, 0],\n",
        "                             [db / (iz * vx), dr / iz, 0, 0, 0],\n",
        "                             [-math.sin(psi), 0, 0, 0, 0],\n",
        "                             [math.cos(psi), 0, 0, 0, 0],\n",
        "                             [0, 1, 0, 0, 0]])\n",
        "\n",
        "        par_mat2 = np.array([[cd * action / m], [dd * action / iz], [vx * math.cos(psi)],\n",
        "                             [vx * math.sin(psi)], [0]], dtype='float64')\n",
        "\n",
        "        var_dot_mat = par_mat1 @ self.vars + par_mat2  # (5,1)= (5,5)@(5,1)+(5,1)\n",
        "        self.vars_tmp = self.vars + dt * var_dot_mat  # (5,1) =(5,1)+(5,1)\n",
        "\n",
        "        if preview == 0:\n",
        "            self.vars_ = self.vars_tmp\n",
        "\n",
        "        return\n",
        "\n",
        "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "    def normalize(self, d, a):\n",
        "        return d/(dist_limit-0), a/(ang_limit1-ang_limit2)\n",
        "\n",
        "    def step(self, action, stp_cnt, pre_point):\n",
        "        self.preview(action, preview=0)\n",
        "        # print(\"____________________________NOT preview_________________________\")\n",
        "        dist, angle_diff, pre_point = lateralenv.dist_diff(self, ep=0, limit_dist=1, limit_ang=0, stp=stp_cnt, pre_point=pre_point)\n",
        "        dist, angle_diff= self.normalize(d=dist, a=angle_diff)\n",
        "        #print(\"140 step :  dist\" , dist, \"angle\", angle_diff, \"Z  \", pre_point)\n",
        "        self.preview(action, preview=1)\n",
        "        # print(\"____________________________preview______________________________\")\n",
        "        future_dist, future_angle_diff, _ = lateralenv.dist_diff(self, ep=0, limit_dist=1, limit_ang=0,\n",
        "                                                                             stp=stp_cnt)\n",
        "        future_dist, future_angle_diff= self.normalize( d= future_dist, a=future_angle_diff)\n",
        "\n",
        "        self.episode_length_cnt = self.episode_length_cnt - 1\n",
        "      \n",
        "        if dist > dist_limit or angle_diff > ang_limit1 or angle_diff<ang_limit2 or self.episode_length_cnt == 0:\n",
        "            self.Done = 1\n",
        "            #return self.vars_, self.state_,  bad_reward, 'nothing' , self.Done, pre_point\n",
        "            return None, None,  bad_reward, 'nothing' , self.Done, None\n",
        "        # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% calc reward %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "        # 3: based on car's vertical disance with the road\n",
        "        else:\n",
        "          weight = 1\n",
        "          action_weight = -10\n",
        "          preview_weight = 0.01\n",
        "          # print(\"point\", point, dist, \"ang\", angle_diff)\n",
        "          # print(\"dist\", dist, \"ang\", angle_diff)\n",
        "          k1 = 1 / (dist ** 2 + angle_diff ** 2)\n",
        "          k2 = 1 / (future_dist ** 2 + future_angle_diff ** 2)\n",
        "          #reward = weight * k1 + preview_weight * k2  # + action_weight * int(action)\n",
        "          reward_calc = f'{weight} * {k1} + {preview_weight}*{k2} + {action_weight} * {action}'\n",
        "          # reward = - angle_diff\n",
        "\n",
        "          ## 4: Sarah test\n",
        "          # ------------------------\n",
        "          reward = k1 * weight + k2 * preview_weight + action_weight * action \n",
        "        \n",
        "          # print(\"dist\", dist,\"angd\", angle_diff, \"p dist\",future_dist, \"p angd\", future_angle_diff)\n",
        "          self.state_ = np.array([dist, angle_diff, future_dist, future_angle_diff])  # real state (not limited)\n",
        "          # self.state_ = np.array([dist, angle_diff]) #real state (not limited)\n",
        "          \n",
        "          # for next step\n",
        "          self.vars = self.vars_\n",
        "          self.coordinates.append(self.vars[2:4, 0])\n",
        "          self.vys.append(self.vars[0])\n",
        "\n",
        "          return self.vars_, self.state_, reward, reward_calc, self.Done, pre_point  # state:(dist, ang_dif)\n",
        "\n",
        "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def render(self, ep, score):\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.plot(self.road_ep.coords.xy[0][:], self.road_ep.coords.xy[1][:], 'r') #road\n",
        "        if ep_length != 0:\n",
        "          plt.plot(np.array(self.coordinates)[:, 0], np.array(self.coordinates)[:, 1], label=score)  # path\n",
        "          b=1\n",
        "        if ep%2 ==0 and b == 1:\n",
        "         #plt.legend()\n",
        "          #plt.show()\n",
        "          plt.savefig(f\"drive/MyDrive/RL_lane_following_debug/paths/path{ep}.jpg\")\n",
        "          plt.cla()  \n",
        "          b=0 \n",
        "        \n",
        "    def reset(self, ep_pointer):  # before each episode\n",
        "        self.Done = 0\n",
        "        self.episode_length_cnt = max_ep_length\n",
        "        self.coordinates = []\n",
        "        self.nearestPiontCheck = []\n",
        "\n",
        "        # a new section of the road excel is selected for each episode\n",
        "        print(\"211ep pointer\", ep_pointer)\n",
        "        data_ep= self.road[ep_pointer:ep_pointer + int(max_ep_length/res), :]\n",
        "        self.road_ep = geom.LineString(zip(self.x[ep_pointer : ep_pointer + int(max_ep_length / res)], \n",
        "                                        self.y[ep_pointer : ep_pointer + int(max_ep_length / res)])) #500*2\n",
        "                                    \n",
        "        # the car starts on the road\n",
        "        st_vy = 0; st_r=0;\n",
        "        st_x = data_ep[0,0] #+ np.random.rand()\n",
        "        #st_x = self.road[ep_pointer:ep_pointer+1,0]\n",
        "        st_y = data_ep[0,1] #+ np.random.rand()\n",
        "        #st_y = self.road[ep_pointer+ep_pointer+1,1]\n",
        "        st_psi = self.heading_angle[ep_pointer] #+ np.random.rand()*0.01\n",
        "        st_pre_point = geom.Point(st_x, st_y)\n",
        "        \n",
        "        self.vars = np.array([[st_vy, st_r, st_x, st_y, st_psi]], dtype='float64').T\n",
        "        self.vars_tmp = np.array([[st_vy, st_r, st_x, st_y, st_psi]], dtype='float64').T # is updated for both normal step and preview step\n",
        "        \n",
        "        #point0_ep = geom.Point(st_x, st_y)\n",
        "        limited_dist0, limited_angle_diff0, _ = self.dist_diff(ep=0, limit_dist=1, limit_ang=0, stp=0)\n",
        "        self.preview(action=0, preview=1)\n",
        "        future_limited_dist0, future_limited_ang0, _ = self.dist_diff(ep=0, limit_dist=1, limit_ang=0, stp=0)  # sefr\n",
        "        \n",
        "        state0_ep = np.array([limited_dist0, limited_angle_diff0, limited_dist0, limited_angle_diff0])  # (1,4)\n",
        "    \n",
        "        return state0_ep, st_pre_point"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = 'drive/MyDrive/RL_lane_following_debug/Chaos-generted road.csv'\n",
        "road = pd.read_csv(uploaded).to_numpy()\n",
        "# r= road[0:100, :]\n",
        "# plt.plot(r[:,0], r[:,1])\n",
        "# print(r[0,0])\n",
        "print(road.shape[0])\n",
        "\n",
        "# print(discrete_road_np.shape)\n",
        "# plt.plot(discrete_road_np[:,0], discrete_road_np[:,1])\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gauHF6SIViZz",
        "outputId": "9e408e65-337f-4daa-c7dc-463379490d72"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw_1Aum5snkq",
        "outputId": "18bc3b39-7737-4675-f123-34706b1a02fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "211ep pointer 0\n",
            "episode 1 ep length  192 score [[283928.75]] avg_score 545143.2\n",
            "211ep pointer 192\n",
            "episode 2 ep length  57 score [[36.053467]] avg_score 272581.88\n",
            "211ep pointer 249\n",
            "episode 3 ep length  94 score [[242.81406]] avg_score 181797.33\n",
            "211ep pointer 343\n",
            "episode 4 ep length  137 score [[342.44705]] avg_score 136465.28\n",
            "211ep pointer 480\n",
            "episode 5 ep length  125 score [[930.0252]] avg_score 109404.734\n",
            "211ep pointer 605\n",
            "episode 6 ep length  215 score [[480.2643]] avg_score 91342.71\n",
            "211ep pointer 820\n",
            "episode 7 ep length  188 score [[282.39996]] avg_score 78369.6\n",
            "211ep pointer 1008\n",
            "episode 8 ep length  284 score [[462.17197]] avg_score 68737.46\n",
            "211ep pointer 1292\n",
            "episode 9 ep length  200 score [[739.0124]] avg_score 61264.188\n",
            "211ep pointer 1492\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "agent = Agent(layer1_dim=128, layer2_dim=64, n_actions=2, alpha_A=0.0003, alpha_C=0.005, gamma=0.99)\n",
        "n_episodes = 100\n",
        "data_length = int(road.shape[0]/10) #10,000\n",
        "max_ep_length = 300 # could be int(data_length / n_episodes) \n",
        "env = lateralenv(road, data_length, n_episodes, max_ep_length)\n",
        "\n",
        "cnt = 0\n",
        "dist_limit = 5\n",
        "ang_limit1 = 0.79; ang_limit2= -0.79; #45 degree\n",
        "bad_reward = 0 \n",
        "res = 8\n",
        "b=0 # for render \n",
        "score_history = []\n",
        "best_score = 0  # reward = 1/positive > 0 -> min score =0\n",
        "load_checkpoint = False\n",
        "\n",
        "workbook = xlsxwriter.Workbook('drive/MyDrive/RL_lane_following_debug/log.xlsx')\n",
        "log = workbook.add_worksheet(\"ep_per_ep\")\n",
        "log.write(0, 0, \"ep / step\")\n",
        "log.write(0, 3, \"vy\")\n",
        "log.write(0, 4, \"point\")\n",
        "log.write(0, 5, \"distance\")\n",
        "log.write(0, 6, \"angle_diff\")\n",
        "log.write(0, 7, \"road derivative\")\n",
        "log.write(0, 8, \"psi\")\n",
        "log.write(0, 9, \"reward\")\n",
        "log.write(0, 10, \"point dist_diff +  preview point dist_diff + action\")\n",
        "\n",
        "# training________________________________________________________________________________________\n",
        "ep_pointer= 0\n",
        "for ep in range(1, n_episodes + 1):\n",
        "    score = 0\n",
        "    al = [];\n",
        "    cl = [];\n",
        "    rewards = []\n",
        "    state, pre_point = env.reset(ep_pointer)  # (1,2)\n",
        "\n",
        "    states_ = []\n",
        "    ep_length= 0\n",
        "    #print(ep, \":____________________________________________________________________________\")\n",
        "    while True:\n",
        "        action = agent.choose_action(state)\n",
        "        # assert action != act_buffer , \"equal actions !!\"\n",
        "        #act_buffer = action\n",
        "\n",
        "        newvars, state_, reward, reward_calc, Done, pre_point = env.step(action, ep_length, pre_point)\n",
        "        if Done==1:\n",
        "          break\n",
        "        # if ep_length >100:\n",
        "        #   break\n",
        "        else:\n",
        "          states_.append(state_)\n",
        "          reward= tf.get_static_value(reward) \n",
        "          score = score + reward\n",
        "          rewards.append(reward)\n",
        "\n",
        "          # if not load_checkpoint:\n",
        "          closs, aloss, grad1 = agent.learn(state, reward, state_, Done)\n",
        "          # log\n",
        "          log.write(ep_pointer + ep_length + 1, 0, f\"{ep} / {ep_length}\")\n",
        "          log.write(ep_pointer + ep_length + 1, 3, newvars[0])\n",
        "          log.write(ep_pointer + ep_length + 1, 4, str(newvars[2:4]))\n",
        "          log.write(ep_pointer + ep_length + 1, 5, state_[0])\n",
        "          log.write(ep_pointer + ep_length + 1, 6, state_[1])\n",
        "          log.write(ep_pointer + ep_length + 1, 7, np.cos(newvars[2] / 200)[0] / 4)\n",
        "          log.write(ep_pointer + ep_length + 1, 8, newvars[-1])\n",
        "          log.write(ep_pointer + ep_length + 1, 9, reward)\n",
        "          log.write(ep_pointer + ep_length + 1, 10, reward_calc)\n",
        "\n",
        "          state = state_\n",
        "          ep_length += 1  # step counter\n",
        "    states_ = np.array(states_)\n",
        "    score_history.append(score* ep_length/100) # ep length should affect score \n",
        "    ep_pointer += ep_length\n",
        "  \n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "    if (ep % 1 == 0):\n",
        "        print('episode', ep, 'ep length ', ep_length, 'score', score, 'avg_score', avg_score)\n",
        "        env.render(ep, score)\n",
        "\n",
        "workbook.close()\n",
        "\n",
        "if not load_checkpoint:\n",
        "    ep = [i + 1 for i in range(n_episodes)]\n",
        "    x = np.array(ep).reshape(n_episodes, 1)\n",
        "    score_history = np.array(score_history).reshape(n_episodes, 1)\n",
        "    plt.xlabel(\"episode\")\n",
        "    plt.ylabel(\"score\")\n",
        "    plt.plot(x, score_history)\n",
        "    plt.savefig('scores.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmPtNVj3Px5c"
      },
      "outputs": [],
      "source": [
        "# test \n",
        "import numpy as np \n",
        "import math\n",
        "import shapely.geometry as geom\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.ops import nearest_points\n",
        "\n",
        "# x= np.arange(0, 10).reshape(10, 1) \n",
        "# y= 50*np.sin(x/200)\n",
        "# road = geom.LineString(zip(x,y))\n",
        "# p= geom.Point(314,50)\n",
        "# print(\"ggg\", p.coords[0][1])\n",
        "# print(\"ggg\", p.coords[0][0])\n",
        "# dist = p.distance(road) \n",
        "# print(dist)\n",
        "# nearestP = nearest_points(road, p)\n",
        "# print(nearestP[0])\n",
        "#angle_diff= np.arctan2(nearestP.centroid.y, nearestP.centroid.x) - self.psi0 #pos/neg mide  \n",
        "\n",
        "n= 25000\n",
        "x= np.arange(0, n).reshape(n, 1) \n",
        "y= 50*np.sin(x/100)\n",
        "plt.plot(x,y)\n",
        "#self.road = geom.LineString(zip(x,y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3CWXkoLX8O5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}