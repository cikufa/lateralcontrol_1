{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cikufa/lateralcontrol_1/blob/shekoufeh/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VVh3mWPIJyp",
        "outputId": "d865d26e-d163-4faa-d5ab-5fb91bdc4aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20 kB 38.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 40 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 81 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 149 kB 7.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.3\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "#choko\n",
        "#tasks: 1-action , preview dist in in reward/ 2- one step ahead in network input/ 3- reward every 5 iteration \n",
        "import numpy as np \n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import shapely.geometry as geom\n",
        "from shapely.ops import nearest_points\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import adam_v2\n",
        "import tensorflow_probability as tfp\n",
        "import os\n",
        "from keras.layers import Dense\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "#from keras.optimizers import adam\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hbQ0B2DcIOs4"
      },
      "outputs": [],
      "source": [
        "class GenericNetwork(keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims, fc2_dims, name, chkpt_dir=\"/tmp/actor_critic\"):\n",
        "        super(GenericNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.fc3 = Dense(n_actions)\n",
        "        \n",
        "        #self.v = Dense(1, activation=None)\n",
        "        #continous action is represented as a normal distribution that is characterized with 2 quantities: a mean and a standard deviation \n",
        "        #self.pi = Dense(n_actions=2, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2e_3NTEAbhqu"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, layer1_dim=128, layer2_dim=64, n_actions=2, alpha_A=0.00003, alpha_C=0.00005, gamma=0.99):\n",
        "        self.layer1_dim = layer1_dim\n",
        "        self.layer2_dim = layer2_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.alpha_A = alpha_A \n",
        "        self.alpha_C= alpha_C\n",
        "        self.action = None\n",
        "        self.log_prob= None\n",
        "        \n",
        "        self.actor = GenericNetwork(n_actions, layer1_dim, layer2_dim, \"actor\")\n",
        "        self.actor.compile(optimizer=adam_v2.Adam(learning_rate=alpha_A))\n",
        "        self.critic = GenericNetwork(1, layer1_dim, layer2_dim, \"critic\")\n",
        "        self.critic.compile(optimizer=adam_v2.Adam(learning_rate=alpha_C))\n",
        "        self.aloss= []\n",
        "        self.closs=[]\n",
        "\n",
        "    def choose_action(self, observation): #obs shape (1,2)\n",
        "        state = tf.convert_to_tensor([observation]) #state shape (1,1,2)\n",
        "        pars= self.actor(state) #mean and standard deviation that make action probs\n",
        "        pars= np.asarray(tf.squeeze(pars)).reshape(1,2)  \n",
        "        sigma , mu = np.hsplit(pars , 2)\n",
        "        sigma = tf.exp(sigma) #get rid of negative sigma\n",
        "        #sigma= abs(sigma)\n",
        "        action_probabilities = tfp.distributions.Normal(mu , sigma) #normal distribution with mu,sigma pars  \n",
        "        #log_prob = action_probabilities.log_prob(action_probabilities) #log (gonna be used for gradient)\n",
        "        action = action_probabilities.sample() #choose action (most likely to be chosen with higher probability)\n",
        "        action = tf.tanh(action) * 0.07 #action: continuous num in range(-0.07, 0.07)((-4,4) degree_\n",
        "        self.action = action  \n",
        "        return action #cast tensor to numpy(openAI gym doesnt take tensor)\n",
        "\n",
        "    def save_models(self):\n",
        "        #print('... saving models ...')\n",
        "        self.actor.save_weights(self.actor.checkpoint_file)\n",
        "        self.critic.save_weights(self.critic.checkpoint_file)\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_weights(self.actor.checkpoint_file)\n",
        "        self.critic.load_weights(self.critic.checkpoint_file)\n",
        "        \n",
        "    def learn(self, state, reward, state_,done):\n",
        "        #print(\"state before \")\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
        "        reward = tf.convert_to_tensor(reward, dtype=tf.float32) # not fed to NN -> no need to reshape\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            state_value = self.critic(state)\n",
        "            state_value_ = self.critic(state_)\n",
        "            state_value = tf.squeeze(state_value) #squeeze Removes dims of size 1 from the shape of a tensor.\n",
        "            state_value_ = tf.squeeze(state_value_)\n",
        "            pars= self.actor(state)\n",
        "            #pars= np.asarray(tf.squeeze(pars)).reshape(1,2)\n",
        "            #mu , sigma= np.hsplit(pars , 2)\n",
        "            #mu = np.squeeze(mu)\n",
        "            #sigma = np.squeeze(sigma)\n",
        "            mu = pars[0,0]\n",
        "            sigma = pars[0,1]\n",
        "            #print(sigma)\n",
        "            #sigma = tf.exp(sigma)\n",
        "            #print(sigma)\n",
        "            action_probs = tfp.distributions.Normal(mu, abs(sigma)) #policy \n",
        "            log_prob = action_probs.log_prob(self.action[0,0] )\n",
        "            #print(mu,sigma)\n",
        "            #print(log_prob)\n",
        "                      \n",
        "            #TD error: \n",
        "            TD= self.gamma*state_value_*(1-int(done)) - state_value \n",
        "            delta = reward + TD #1-done: terminal stRemoves dimensions of size 1 from the shape of a tensor.ate zero effect \n",
        "            actor_loss = (-log_prob*delta)            \n",
        "            critic_loss = (delta**2) \n",
        "            #print(\"sig\", sigma , \"ac\", actor_loss, \"cr\", critic_loss)\n",
        "  \n",
        "            \n",
        "        gradient1 = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        \n",
        "        self.actor.optimizer.apply_gradients((grad , var) for (grad , var) in zip(gradient1, self.actor.trainable_variables) if grad is not None)\n",
        "        #if grad is not None\n",
        "            \n",
        "        gradient2 = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients((grad , var) for (grad , var) in zip(gradient2, self.critic.trainable_variables) if grad is not None)\n",
        "        # if grad is not None\n",
        "        return critic_loss, actor_loss, gradient1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-7jAXNqRskpg"
      },
      "outputs": [],
      "source": [
        "class lateralenv:\n",
        "    def __init__(self, roadfile, data_length , n_episodes, episode_length):\n",
        "        #constants\n",
        "        dt=0.01\n",
        "        vx=10\n",
        "        iz= 2278.8\n",
        "        m=1300\n",
        "        a1=1; a2=1.5\n",
        "        caf = 60000; car= 60000\n",
        "        cb= -(caf + car); cr= (-a1*caf + a2*car)/ vx\n",
        "        db= -(a1* caf - a2*car); dr= -(a1**2 *caf + a2**2*car) / vx\n",
        "        cd = caf; dd= a1*caf\n",
        "        self.constants=[dt, vx, iz, m, cb, cr, db, dr, cd, dd]\n",
        "       \n",
        "        self.data_length = data_length \n",
        "        self.n_episodes = n_episodes \n",
        "        self.episode_length=episode_length\n",
        "        self.episode_length_cnt =episode_length\n",
        "       \n",
        "        #x = np.linspace(0, 10*math.pi, 20000).reshape(20000,1) #0<x<314\n",
        "        x= np.arange(0, self.data_length).reshape(self.data_length, 1) \n",
        "        y= 50*np.sin(x/200)\n",
        "        self.road = geom.LineString(zip(x,y))\n",
        "        #self.road = genfromtxt(roadfile, delimiter=',')\n",
        "        heading_angle = [np.arctan2(y[i+1]-y[i] , x[i+1]-x[i]) for i in range(y.shape[0]-1)] #rad #56.3\n",
        "        heading_angle.insert(0,heading_angle[0]) #append last value to adjust the shape         \n",
        "        heading_angle= np.asfarray(heading_angle).reshape(self.data_length,1)\n",
        "\n",
        "        #print('_psiiiiiii', heading_angle[0])\n",
        "\n",
        "        #______________________________________________init vars_____________________________________________________________\n",
        "        self.vy0=0; self.r0=0; self.x0= 3; self.y0= 0; self.psi0= heading_angle[0] \n",
        "        self.vars0= np.array([[self.vy0, self.r0, self.x0, self.y0, self.psi0]], dtype='float64').T #1,5 vars0\n",
        "        self.point0 = geom.Point(self.x0, self.y0)\n",
        "        self.vars= self.vars0\n",
        "        self.vars_= np.zeros((5,1))\n",
        "        self.score = 0\n",
        "        self.index=0\n",
        "        self.Done=0\n",
        "        self.coordinates=[]\n",
        "        self.vys = []\n",
        "        self.vymax = -10\n",
        "\n",
        "        point0 = geom.Point(self.x0, self.y0)\n",
        "        limited_dist0,limited_angle_diff0, _ = self.dist_diff(point=point0, ep=0 , limit=1, stp=0)\n",
        "        preview_point0= self.preview(action, preview=1)\n",
        "        future_limited_dist0, future_limited_ang0 ,_ = self.dist_diff(point=preview_point0, ep=0, limit=1, stp=0) # sefr\n",
        "        #self.state0= np.array([limited_dist0,limited_angle_diff0]) #(1,2)\n",
        "        self.state0= np.array([limited_dist0,limited_angle_diff0, limited_dist0,limited_angle_diff0]) #(1,4)        \n",
        "        \n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def dist_diff(self, point,ep, limit, stp):\n",
        "               \n",
        "        ##1: based on where the car is supposed to be on this iteration\n",
        "        #self.index = self.index+1\n",
        "        #dist = self.data[self.index, 0:2] - coordinate_ #(1,2)\n",
        "        #angle_diff= self.data[self.index,2] - psi_  \n",
        "        #________________________________________________________________________\n",
        "        \n",
        "        ## 2: based on where the car is supposed to be if the driven distance was along the road\n",
        "        # driving_distance= (vy**2 + vx**2)**0.5 *dt #driving angle : psi_ \n",
        "        # #(dx^2+dy^2)^0.5= distance , dy=1.5dx -> (3.25dx^2)*0.5 = distance -> 1.802 dx = distance\n",
        "        # dx= driving_distance / math.sqrt(3.25); dy= dx*1.5\n",
        "        # dist= ((dx - x_)**2 + (dy- y_)**2)**0.5  \n",
        "        #angle_diff =\n",
        "        #_______________________________________________________________________\n",
        "\n",
        "        #3: based on car's vertical disance with the road          \n",
        "        dist = point.distance(self.road) \n",
        "        dist_z = math.sqrt((point.coords[0][1]-self.y0)**2+(point.coords[0][0]-self.x0)**2)\n",
        "        limited_dist= max(dist, 0.01)\n",
        "        limited_dist= min(limited_dist, 100)\n",
        "\n",
        "        nearestP = nearest_points(self.road, point)[0]\n",
        "        angle_diff= np.arctan2(nearestP.centroid.y, nearestP.centroid.x) - np.arctan2(point.y,point.x) #pos/neg mide        \n",
        "        limited_angle_diff=max(angle_diff, 0.01)\n",
        "        limited_angle_diff=min(limited_angle_diff , 100) #-> max reward = 5000, min reward 5e-5\n",
        "        \n",
        "        #debug\n",
        "        # p_buffer=np.zeros((500, 500)) ; dist_buffer=np.zeros((500, 500,3))\n",
        "        # assert p_buffer[ep][stp][0] != point.coords[0][0] , \"equal points !!\"\n",
        "        # assert dist_buffer[ep][stp] != [dist, angle_diff, dist_z] , 'equal dist !!'\n",
        "        # p_buffer[ep][stp]= point.coords[0][0]\n",
        "        # dist_buffer[ep][stp]= [dist, angle_diff, dist_z]\n",
        "        \n",
        "        #print(\"point\", point)\n",
        "        #print(\"dist\" , dist, \"angle\", angle_diff, \"Z  \", dist_z)\n",
        "\n",
        "        if limit==1:\n",
        "          return limited_dist,limited_angle_diff,dist_z\n",
        "        else: #limit=0\n",
        "          return dist,angle_diff,dist_z\n",
        "\n",
        "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def preview(self, action, preview): # in this version the preview point is calculated using the updated self.vars. also try with non-updated vars\n",
        "        dt, vx, iz, m, cb, cr, db, dr, cd, dd= self.constants\n",
        "        vy, r, x, y, psi = np.vsplit(self.vars,5)\n",
        "        if preview==1:\n",
        "          dt = 0.1\n",
        "        if preview == 0:\n",
        "          dt= 0.3 \n",
        "        #calc new state\n",
        "        par_mat1 = np.array([[cb/(m*vx), cr/m-vx,0,0,0],\n",
        "                           [db/(iz*vx), dr/iz, 0,0,0],\n",
        "                           [-math.sin(psi),0,0,0,0],\n",
        "                           [math.cos(psi),0,0,0,0],\n",
        "                           [0,1,0,0,0]])\n",
        "        \n",
        "        par_mat2 = np.array([[cd* action /m],[dd*action/iz], [vx*math.cos(psi)],\n",
        "                    [vx*math.sin(psi)],[0]], dtype='float64') \n",
        "     \n",
        "        var_dot_mat = par_mat1 @ self.vars + par_mat2  #(5,1)= (5,5)@(5,1)+(5,1)\n",
        "        self.vars_= self.vars + dt* var_dot_mat #(5,1) =(5,1)+(5,1)\n",
        "        \n",
        "        vy_, r_, x_, y_, psi_= np.vsplit(self.vars_,5)\n",
        "        #print(\"vy , vy_\", self.vars[0], self.vars_[0])\n",
        "        # if self.vars_[0] > self.vymax:\n",
        "        #     self.vymax = self.vars_[0]\n",
        "#        print(\"max vy: \", self.vymax)\n",
        "\n",
        "        ##calc reward\n",
        "        ##1: based on where the car is supposed to be on this iteration\n",
        "        #self.index = self.index+1\n",
        "        #dist = self.data[self.index, 0:2] - coordinate_ #(1,2)\n",
        "        #angle_diff= self.data[self.index,2] - psi_  \n",
        "        #________________________________________________________________________\n",
        "        \n",
        "        ## 2: based on where the car is supposed to be if the driven distance was along the road\n",
        "        # driving_distance= (vy**2 + vx**2)**0.5 *dt #driving angle : psi_ \n",
        "        # #(dx^2+dy^2)^0.5= distance , dy=1.5dx -> (3.25dx^2)*0.5 = distance -> 1.802 dx = distance\n",
        "        # dx= driving_distance / math.sqrt(3.25); dy= dx*1.5\n",
        "        # dist= ((dx - x_)**2 + (dy- y_)**2)**0.5  \n",
        "        #angle_diff =\n",
        "         #_______________________________________________________________________\n",
        "\n",
        "        #3: based on car's vertical disance with the road   \n",
        "\n",
        "        #_______________________________________________________________________\n",
        "         \n",
        "        point_ = geom.Point(x_, y_)\n",
        "        if preview==1:\n",
        "          #print(\"pre\", point_)\n",
        "          return point_\n",
        "        if preview==0:\n",
        "          #print(\"not pre\", point_)\n",
        "          return self.vars_, point_\n",
        "     \n",
        "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def step(self, point, action,stp_cnt): \n",
        "        vars, point_ = self.preview(action, preview=0)\n",
        "        # print(\"step\", point)\n",
        "        # print(\"____________________________NOT preview_________________________\")\n",
        "        dist, angle_diff, dist_z = lateralenv.dist_diff(self, point_,ep, 0, stp_cnt)\n",
        "        #print(\"step :  dist\" , dist, \"angle\", angle_diff, \"Z  \", dist_z)\n",
        "        preview_point = self.preview(action, preview=1)\n",
        "        # print(\"____________________________preview______________________________\")\n",
        "        future_dist, future_angle_diff, future_dist_z = lateralenv.dist_diff(self, preview_point,ep, 0, stp_cnt)        \n",
        "        # print(stp_cnt)\n",
        "\n",
        "        reward = - dist_z*future_dist_z/10000\n",
        "        #reward = - dist_z/100\n",
        "        #action_weight = 10      \n",
        "        #preview_weight = 0.01\n",
        "        #reward = 1/(dist**2 + ang_diff**2) # + preview_weight/(future_dist**2 + future_ang**2) #+ action * action_weight\n",
        "               \n",
        "        # if dist > 2:\n",
        "        #     makhraj = -1/dist\n",
        "        # #try: \n",
        "        # else:\n",
        "        #     min = 1e-1\n",
        "        #     max = 1e+1\n",
        "        #     makhraj= abs(dist)**2 if abs(dist)**2 > min else min #max reward= 10\n",
        "            \n",
        "        '''except OverflowError as err:\n",
        "            makhraj = max #min reward = 0.1'''\n",
        "\n",
        "        #for next step\n",
        "        #self.coordinates.append(self.vars[2:4,0])\n",
        "        #self.vys.append(self.vars[0])\n",
        "        #self.vars = self.vars_ \n",
        "        self.state_ = np.array([dist, angle_diff, future_dist, future_angle_diff]) #real state (not limited) \n",
        "        #self.state_ = np.array([dist, angle_diff]) #real state (not limited) \n",
        "\n",
        "        self.vars = self.vars_\n",
        "        self.coordinates.append(self.vars[2:4,0])\n",
        "        self.vys.append(self.vars[0])\n",
        "        \n",
        "        self.episode_length_cnt = self.episode_length_cnt -1\n",
        "        if self.episode_length_cnt==0: \n",
        "            self.Done = 1\n",
        "\n",
        "        return point_, self.state_, reward, self.Done #state:(dist, ang_dif)\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "    \n",
        "    def render(self, ep, score):\n",
        "        #plt.subplot(1,2,2)\n",
        "        #worksheet.write(ep+1, 0, int(aloss))\n",
        "        #worksheet.write(ep+1, 1, int(closs))\n",
        "        #worksheet.write(ep+1, 2, score) \n",
        "        \n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.plot(self.road.coords.xy[0][0:int(self.episode_length)*3], self.road.coords.xy[1][0:int(self.episode_length)*3],'r') #road \n",
        "        plt.plot(np.array(self.coordinates)[:,0], np.array(self.coordinates)[:,1], label =score) #path\n",
        "        plt.legend()\n",
        "        if (ep%10 == 0):\n",
        "          # plt.xlabel(\"x\")\n",
        "          # plt.ylabel(\"y\")\n",
        "          # plt.plot(self.data[0:self.episode_length*2, 0], self.data[0:self.episode_length*2,1],'r') #road \n",
        "          # plt.plot(np.array(self.coordinates)[:,0], np.array(self.coordinates)[:,1], label =score) #path\n",
        "          # plt.legend()\n",
        "          plt.savefig(f\"drive/MyDrive/RL_lane_following_debug/path{ep}.jpg\")\n",
        "          plt.cla() \n",
        "          #if (ep%50 ==0):\n",
        "        pass\n",
        "    \n",
        "    def reset(self): # before each episode \n",
        "        self.Done= 0\n",
        "        self.episode_length_cnt = self.episode_length \n",
        "        \n",
        "        #p = geom.Point(self.vars_[2,0], self.vars_[3,0])\n",
        "        #init_coord = nearest_points(self.road, p)[0]\n",
        "        #self.state = np.hstack((init_coord, math.atan(1.5)))\n",
        "        #self.state = np.array([[0,0,math.atan(1.5)]]) #shape(1,3)\n",
        "        \n",
        "        self.vars= self.vars0\n",
        "        self.coordinates = []\n",
        "        return self.state0, self.point0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw_1Aum5snkq",
        "outputId": "292b5b43-6662-45ba-ed4a-3c4a1825880c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 :____________________________________________________________________________\n",
            "episode  1 score -463.702369563142 avg_score -463.702369563142 reward -2.7451750724103063\n",
            "2 :____________________________________________________________________________\n",
            "episode  2 score -320.4201400041244 avg_score -392.0612547836332 reward -0.39517981107887273\n",
            "3 :____________________________________________________________________________\n",
            "episode  3 score -602.5514361866866 avg_score -462.224648584651 reward -5.252148356638016\n",
            "4 :____________________________________________________________________________\n",
            "episode  4 score -97.97504668365188 avg_score -371.1622481094012 reward -0.13790970679660142\n",
            "5 :____________________________________________________________________________\n",
            "episode  5 score -171.17134857655284 avg_score -331.1640682028316 reward -1.6708363167867952\n",
            "6 :____________________________________________________________________________\n",
            "episode  6 score -109.77979494529923 avg_score -294.26668932657617 reward -0.1407073899416366\n",
            "7 :____________________________________________________________________________\n",
            "episode  7 score -101.3908915419962 avg_score -266.713003928779 reward -0.13935957699230875\n",
            "8 :____________________________________________________________________________\n",
            "episode  8 score -100.15080046263591 avg_score -245.89272849551114 reward -0.13768543050341342\n",
            "9 :____________________________________________________________________________\n",
            "episode  9 score -104.2022011396074 avg_score -230.14933656707737 reward -0.140725169451637\n",
            "10 :____________________________________________________________________________\n",
            "episode  10 score -105.57474534456796 avg_score -217.69187744482642 reward -0.14068851628152265\n",
            "11 :____________________________________________________________________________\n",
            "episode  11 score -109.9555821281439 avg_score -207.89766877967347 reward -0.14068075776907116\n",
            "12 :____________________________________________________________________________\n",
            "episode  12 score -99.61543324297108 avg_score -198.87414915161492 reward -0.13876623882761618\n",
            "13 :____________________________________________________________________________\n",
            "episode  13 score -105.41253188777948 avg_score -191.68479397747373 reward -0.14068079130408997\n",
            "14 :____________________________________________________________________________\n",
            "episode  14 score -99.43784294755963 avg_score -185.09572604676558 reward -0.13857016005395648\n",
            "15 :____________________________________________________________________________\n",
            "episode  15 score -102.1407953644768 avg_score -179.56539733461298 reward -0.13963665745571183\n",
            "16 :____________________________________________________________________________\n",
            "episode  16 score -102.5598556987838 avg_score -174.7525509823737 reward -0.14014212291786857\n",
            "17 :____________________________________________________________________________\n",
            "episode  17 score -100.76292438706433 avg_score -170.400220006179 reward -0.13906935336113474\n",
            "18 :____________________________________________________________________________\n",
            "episode  18 score -105.6720844409802 avg_score -166.8042124747791 reward -0.14059726429069336\n",
            "19 :____________________________________________________________________________\n",
            "episode  19 score -427.5777934152355 avg_score -180.52913778743468 reward -1.4639439183953857\n",
            "20 :____________________________________________________________________________\n",
            "episode  20 score -168.20225347876735 avg_score -179.91279357200133 reward -0.13239467931778706\n",
            "21 :____________________________________________________________________________\n",
            "episode  21 score -171.3964215862252 avg_score -179.50725204886913 reward -0.13259329504772652\n",
            "22 :____________________________________________________________________________\n",
            "episode  22 score -185.15424680247787 avg_score -179.7639336285786 reward -0.14069189749923394\n",
            "23 :____________________________________________________________________________\n",
            "episode  23 score -188.86396954574934 avg_score -180.15958736410775 reward -0.1434419388302172\n",
            "24 :____________________________________________________________________________\n",
            "episode  24 score -194.6005183669177 avg_score -180.7612928225582 reward -0.14763439783783106\n",
            "25 :____________________________________________________________________________\n",
            "episode  25 score -184.84564019740836 avg_score -180.9246667175522 reward -0.14052712232432105\n",
            "26 :____________________________________________________________________________\n",
            "episode  26 score -205.43137585718677 avg_score -181.867232453692 reward -0.15574793132423984\n",
            "27 :____________________________________________________________________________\n",
            "episode  27 score -195.18469160175775 avg_score -182.36047168139814 reward -0.14807678115522413\n",
            "28 :____________________________________________________________________________\n",
            "episode  28 score -193.81128616610363 avg_score -182.7694293415662 reward -0.14715504975526386\n",
            "29 :____________________________________________________________________________\n",
            "episode  29 score -197.11296004771313 avg_score -183.26403384867473 reward -0.15003802900630697\n",
            "30 :____________________________________________________________________________\n",
            "episode  30 score -191.6292594701491 avg_score -183.54287470272388 reward -0.1460099332843173\n",
            "31 :____________________________________________________________________________\n",
            "episode  31 score -199.03771559681542 avg_score -184.04270827995262 reward -0.15058077682442417\n",
            "32 :____________________________________________________________________________\n",
            "episode  32 score -207.04512379223854 avg_score -184.76153376471154 reward -0.1585449290987438\n",
            "33 :____________________________________________________________________________\n",
            "episode  33 score -197.06085895864285 avg_score -185.13424058877007 reward -0.14917369450835286\n",
            "34 :____________________________________________________________________________\n",
            "episode  34 score -208.43306936609775 avg_score -185.81950025869148 reward -0.1577766334689884\n",
            "35 :____________________________________________________________________________\n",
            "episode  35 score -226.59456908665734 avg_score -186.98450222520478 reward -0.1771233458265808\n",
            "36 :____________________________________________________________________________\n",
            "episode  36 score -231.886990579717 avg_score -188.23179356838568 reward -0.18351452668223134\n",
            "37 :____________________________________________________________________________\n",
            "episode  37 score -223.16906027695197 avg_score -189.17604401996857 reward -0.1736330847105531\n",
            "38 :____________________________________________________________________________\n",
            "episode  38 score -191.871791200244 avg_score -189.24698473523895 reward -0.13130802794653992\n",
            "39 :____________________________________________________________________________\n",
            "episode  39 score -272.44827359537084 avg_score -191.38035111626797 reward -0.24138509717251883\n",
            "40 :____________________________________________________________________________\n",
            "episode  40 score -330.5095119888589 avg_score -194.85858013808274 reward -0.41060478315706306\n",
            "41 :____________________________________________________________________________\n",
            "episode  41 score -96.74435509478036 avg_score -192.4655502589778 reward -0.1388683959984761\n",
            "42 :____________________________________________________________________________\n",
            "episode  42 score -96.81824293641102 avg_score -190.18823341796428 reward -0.13971211994862398\n",
            "43 :____________________________________________________________________________\n",
            "episode  43 score -96.75321405375638 avg_score -188.0153259908897 reward -0.13882927610882667\n",
            "44 :____________________________________________________________________________\n",
            "episode  44 score -96.77415512293348 avg_score -185.94166301661795 reward -0.1389378771552614\n",
            "45 :____________________________________________________________________________\n",
            "episode  45 score -96.72618002503307 avg_score -183.9590967279161 reward -0.13908401914698518\n",
            "46 :____________________________________________________________________________\n",
            "episode  46 score -96.71242453261966 avg_score -182.0624299410618 reward -0.13900724857849242\n",
            "47 :____________________________________________________________________________\n",
            "episode  47 score -96.76341238090473 avg_score -180.2475572270159 reward -0.1390417206315116\n",
            "48 :____________________________________________________________________________\n",
            "episode  48 score -96.72866235800778 avg_score -178.50758025057826 reward -0.13872401236022566\n",
            "49 :____________________________________________________________________________\n",
            "episode  49 score -96.72630421418997 avg_score -176.83857461718256 reward -0.13874874864809886\n",
            "50 :____________________________________________________________________________\n",
            "episode  50 score -96.75123646890565 avg_score -175.23682785421704 reward -0.13870955078314623\n",
            "51 :____________________________________________________________________________\n",
            "episode  51 score -96.69259054451443 avg_score -173.6967447697131 reward -0.13867237776704575\n",
            "52 :____________________________________________________________________________\n",
            "episode  52 score -96.7546081337091 avg_score -172.2170882959438 reward -0.1388799187013577\n",
            "53 :____________________________________________________________________________\n",
            "episode  53 score -96.6939369543145 avg_score -170.7921231762904 reward -0.13881147271789276\n",
            "54 :____________________________________________________________________________\n",
            "episode  54 score -96.6846604546206 avg_score -169.41976275551875 reward -0.13862884577107917\n",
            "55 :____________________________________________________________________________\n",
            "episode  55 score -96.68196595091653 avg_score -168.09725735907142 reward -0.13862005745024614\n",
            "56 :____________________________________________________________________________\n",
            "episode  56 score -96.70096757889583 avg_score -166.82232361299683 reward -0.1385107435271925\n",
            "57 :____________________________________________________________________________\n",
            "episode  57 score -96.72800569569202 avg_score -165.59259873725463 reward -0.13856133740339618\n",
            "58 :____________________________________________________________________________\n",
            "episode  58 score -96.68368614840772 avg_score -164.40451403744694 reward -0.13847855800315828\n",
            "59 :____________________________________________________________________________\n",
            "episode  59 score -96.73531934463314 avg_score -163.2575785341789 reward -0.1384909755752848\n",
            "60 :____________________________________________________________________________\n",
            "episode  60 score -96.6585834994978 avg_score -162.14759528360088 reward -0.1383058248478972\n",
            "61 :____________________________________________________________________________\n",
            "episode  61 score -96.71158094765833 avg_score -161.07487373711 reward -0.1383605328201608\n",
            "62 :____________________________________________________________________________\n",
            "episode  62 score -96.69740280882549 avg_score -160.03652743181513 reward -0.13838727859358108\n",
            "63 :____________________________________________________________________________\n",
            "episode  63 score -96.72445511495329 avg_score -159.03157390297602 reward -0.137669205295752\n",
            "64 :____________________________________________________________________________\n",
            "episode  64 score -96.44615981910832 avg_score -158.05367680791562 reward -0.13783747031802243\n",
            "65 :____________________________________________________________________________\n",
            "episode  65 score -96.42370026747192 avg_score -157.10552332267804 reward -0.13756301107369795\n",
            "66 :____________________________________________________________________________\n",
            "episode  66 score -96.60725144542295 avg_score -156.1888828396893 reward -0.13753688279870263\n",
            "67 :____________________________________________________________________________\n",
            "episode  67 score -96.59449819733402 avg_score -155.29941441219148 reward -0.13750314749089862\n",
            "68 :____________________________________________________________________________\n",
            "episode  68 score -96.56170231211517 avg_score -154.43562452836682 reward -0.13741620014387412\n",
            "69 :____________________________________________________________________________\n",
            "episode  69 score -96.57493243744574 avg_score -153.59706377342596 reward -0.13740203587232197\n",
            "70 :____________________________________________________________________________\n",
            "episode  70 score -96.55839180664864 avg_score -152.782225602472 reward -0.1373429244080559\n",
            "71 :____________________________________________________________________________\n",
            "episode  71 score -96.55962338257865 avg_score -151.9903579655721 reward -0.13730495332423048\n",
            "72 :____________________________________________________________________________\n",
            "episode  72 score -96.54133746297377 avg_score -151.22023268081375 reward -0.13726199305924652\n",
            "73 :____________________________________________________________________________\n",
            "episode  73 score -96.4919712910423 avg_score -150.47053046999494 reward -0.13713389223448974\n",
            "74 :____________________________________________________________________________\n",
            "episode  74 score -96.4579442017166 avg_score -149.74063065555876 reward -0.1370615791027252\n",
            "75 :____________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "agent = Agent(layer1_dim=128, layer2_dim=64, n_actions=2, alpha_A=0.0003, alpha_C=0.005, gamma=0.99)\n",
        "n_episodes = 100\n",
        "data_length= 30000 \n",
        "ep_length = int(data_length/n_episodes) #500\n",
        "roadfile = 'drive/MyDrive/RL_lane_following_debug/RealRoad1.csv'\n",
        "#env = lateralenv(roadfile, data_length, n_episodes, ep_length)\n",
        "env = lateralenv(roadfile, data_length, n_episodes, ep_length)\n",
        "\n",
        "score_history = []\n",
        "best_score = 0 #reward = 1/positive > 0 -> min score =0    \n",
        "load_checkpoint = False \n",
        "if load_checkpoint:\n",
        "    agent.load_models()\n",
        "\n",
        "#data_path= os.path.join(os.path.abspath(os.getcwd()), \"/Road Samples\")\n",
        "workbook = xlsxwriter.Workbook('drive/MyDrive/RL_lane_following_debug/log.xlsx')\n",
        "log = workbook.add_worksheet(\"ep_per_ep\")\n",
        "log.write(0,0,\"ep / step\")\n",
        "log.write(0,1,\"actor loss\")\n",
        "log.write(0,2,\"critic loss\")\n",
        "log.write(0,3,\"reward\")\n",
        "log.write(0,4,\"distance\")\n",
        "log.write(0,5,\"angle_diff\")\n",
        "\n",
        "#training________________________________________________________________________________________\n",
        "cnt=0\n",
        "for ep in range(1, n_episodes+1): \n",
        "    #file= open(\"/%d.csv\".format(i//4))  \n",
        "    #roadxy = np.loadtxt(file, delimiter=\" \")\n",
        "    #observation = roadxy[roadxy.shape[0]/4*i%4,roadxy.shape[0]//4*(i%4+1)]\n",
        "    #data_chunk = env.data[episode_length * i:episode_length * (i+1) , :]\n",
        "    score=0\n",
        "    al = []; cl=[]; rewards=[]\n",
        "    state, point = env.reset() #(1,2)\n",
        "    #for j in range(2):\n",
        "    j=0\n",
        "    states_=[]\n",
        "    act_buffer= 0\n",
        "    print(ep, \":____________________________________________________________________________\")\n",
        "    while not env.Done:\n",
        "        action = agent.choose_action(state)\n",
        "        #assert action != act_buffer , \"equal actions !!\" \n",
        "        act_buffer= action\n",
        "        \n",
        "        point_, state_, reward, Done = env.step(point, action,j)\n",
        "        #print(\"reward\", reward)\n",
        "        states_.append(state_)\n",
        "        \n",
        "        score = score + reward\n",
        "        rewards.append(reward)\n",
        "\n",
        "        #if not load_checkpoint:\n",
        "        closs, aloss, grad1= agent.learn(state, reward, state_, Done)\n",
        "        #log\n",
        "        log.write((ep-1)*ep_length + j+1, 0, f\"{ep} / {j}\")\n",
        "        log.write((ep-1)*ep_length + j+1, 1, str(closs))\n",
        "        log.write((ep-1)*ep_length + j+1, 2, str(aloss))\n",
        "        log.write((ep-1)*ep_length + j+1, 3, reward)\n",
        "        log.write((ep-1)*ep_length + j+1, 4, state_[0])\n",
        "        log.write((ep-1)*ep_length + j+1, 5, state_[1])\n",
        "        #log.write((ep-1)*ep_length + j+1, 6, int(grad1))\n",
        "\n",
        "        state = state_\n",
        "        point = point_\n",
        "        j+=1 #step counter\n",
        "        #print(state_)\n",
        "\n",
        "    states_ = np.array(states_)\n",
        "    # m=np.min(states_, axis=0)\n",
        "    # M=np.max(states_, axis=0)\n",
        "    # print(\"episode \", j, \"min\", m, \"max\", M)\n",
        "    \n",
        "    #print(\"max\", states_[0].max())\n",
        "    score_history.append(score)\n",
        "  \n",
        "        #______________ plot score curve)\n",
        "        # ep = [i+1 for i in range(i)]\n",
        "        # x= np.array(ep).reshape(i,1)\n",
        "        # #score_history= np.array(score_history).reshape(i,1)\n",
        "        # plt.xlabel(\"episode\")\n",
        "        # plt.ylabel(\"score\")\n",
        "        # plt.plot(x, np.array(score_history).reshape(i,1))\n",
        "        # plt.savefig(figure_file)\n",
        "\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        if not load_checkpoint:\n",
        "            agent.save_models()\n",
        "    if (ep%1==0):\n",
        "        print('episode ', ep, 'score' ,score, 'avg_score' ,avg_score, 'reward', reward)\n",
        "        env.render(ep, score)\n",
        "   \n",
        "workbook.close()\n",
        "\n",
        "if not load_checkpoint:\n",
        "    ep = [i+1 for i in range(n_episodes)]\n",
        "    x= np.array(ep).reshape(n_episodes,1)\n",
        "    score_history= np.array(score_history).reshape(n_episodes,1)\n",
        "    plt.xlabel(\"episode\")\n",
        "    plt.ylabel(\"score\")\n",
        "    plt.plot(x, score_history)\n",
        "    plt.savefig('scores.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd5X-6pIQ8vU"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmPtNVj3Px5c",
        "outputId": "4f6c27f8-3ad5-4fc8-eaed-32e50b6d5287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggg 50.0\n",
            "ggg 314.0\n",
            "308.71529766687416\n",
            "POINT (9 2.2492407018830116)\n"
          ]
        }
      ],
      "source": [
        "# test \n",
        "import numpy as np \n",
        "import math\n",
        "import shapely.geometry as geom\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.ops import nearest_points\n",
        "\n",
        "x= np.arange(0, 10).reshape(10, 1) \n",
        "y= 50*np.sin(x/200)\n",
        "road = geom.LineString(zip(x,y))\n",
        "p= geom.Point(314,50)\n",
        "print(\"ggg\", p.coords[0][1])\n",
        "print(\"ggg\", p.coords[0][0])\n",
        "dist = p.distance(road) \n",
        "print(dist)\n",
        "nearestP = nearest_points(road, p)\n",
        "print(nearestP[0])\n",
        "#angle_diff= np.arctan2(nearestP.centroid.y, nearestP.centroid.x) - self.psi0 #pos/neg mide         \n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlAwco_gsuPK"
      },
      "outputs": [],
      "source": [
        "class t:\n",
        "    def __init__(self, roadfile, data_length , n_episodes, episode_length):\n",
        "        #constants\n",
        "        dt=1 #0.1\n",
        "        vx=10\n",
        "        iz= 2278.8\n",
        "        m=1300\n",
        "        a1=1; a2=1.5\n",
        "        caf = 60000; car= 60000\n",
        "        cb= -(caf + car); cr= (-a1*caf + a2*car)/ vx\n",
        "        db= -(a1* caf - a2*car); dr= -(a1**2 *caf + a2**2*car) / vx\n",
        "        cd = caf; dd= a1*caf\n",
        "        self.constants=[dt, vx, iz, m, cb, cr, db, dr, cd, dd]\n",
        "         \n",
        "    def preview(self, point):\n",
        "        action = 0\n",
        "        dt, vx, iz, m, cb, cr, db, dr, cd, dd= self.constants\n",
        "        vy, r, x, y, psi = np.vsplit(self.vars,5)\n",
        "        #dt = dt*5 # 5 step forward preview \n",
        "        #calc new state\n",
        "        par_mat1 = np.array([[cb/(m*vx), cr/m-vx,0,0,0],\n",
        "                           [db/(iz*vx), dr/iz, 0,0,0],\n",
        "                           [-math.sin(psi),0,0,0,0],\n",
        "                           [math.cos(psi),0,0,0,0],\n",
        "                           [0,1,0,0,0]])\n",
        "        \n",
        "        par_mat2 = np.array([[cd* action /m],[dd*action/iz], [vx*math.cos(psi)],\n",
        "                    [vx*math.sin(psi)],[0]], dtype='float64') \n",
        "     \n",
        "        var_dot_mat = par_mat1 @ self.vars + par_mat2  #(5,1)= (5,5)@(5,1)+(5,1)\n",
        "        self.vars_= self.vars + dt* var_dot_mat #(5,1) =(5,1)+(5,1)\n",
        "        vy_, r_, x_, y_, psi_= np.vsplit(self.vars_,5)\n",
        "        future_point= geom.Point(x_, y_)\n",
        "\n",
        "        return future_point\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b5wsD1rXOYR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "test.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}