{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cikufa/lateralcontrol_1/blob/shekoufeh/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VVh3mWPIJyp",
        "outputId": "5a5e760c-657a-454f-8921-0a90b0fcc955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 5.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.3\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "#choko\n",
        "#tasks: 1-action , preview dist in in reward/ 2- one step ahead in network input/ 3- reward every 5 iteration \n",
        "import numpy as np \n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import shapely.geometry as geom\n",
        "from shapely.ops import nearest_points\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import adam_v2\n",
        "import tensorflow_probability as tfp\n",
        "import os\n",
        "from keras.layers import Dense\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "#from keras.optimizers import adam\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbQ0B2DcIOs4"
      },
      "outputs": [],
      "source": [
        "class GenericNetwork(keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims, fc2_dims, name, chkpt_dir=\"/tmp/actor_critic\"):\n",
        "        super(GenericNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.fc3 = Dense(n_actions)\n",
        "        \n",
        "        #self.v = Dense(1, activation=None)\n",
        "        #continous action is represented as a normal distribution that is characterized with 2 quantities: a mean and a standard deviation \n",
        "        #self.pi = Dense(n_actions=2, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e_3NTEAbhqu"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, layer1_dim=128, layer2_dim=64, n_actions=2, alpha_A=0.00003, alpha_C=0.00005, gamma=0.99):\n",
        "        self.layer1_dim = layer1_dim\n",
        "        self.layer2_dim = layer2_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.alpha_A = alpha_A \n",
        "        self.alpha_C= alpha_C\n",
        "        self.action = None\n",
        "        self.log_prob= None\n",
        "        \n",
        "        self.actor = GenericNetwork(n_actions, layer1_dim, layer2_dim, \"actor\")\n",
        "        self.actor.compile(optimizer=adam_v2.Adam(learning_rate=alpha_A))\n",
        "        self.critic = GenericNetwork(1, layer1_dim, layer2_dim, \"critic\")\n",
        "        self.critic.compile(optimizer=adam_v2.Adam(learning_rate=alpha_C))\n",
        "        self.aloss= []\n",
        "        self.closs=[]\n",
        "\n",
        "    def choose_action(self, observation): #obs shape (1,2)\n",
        "        state = tf.convert_to_tensor([observation]) #state shape (1,1,2)\n",
        "        pars= self.actor(state) #mean and standard deviation that make action probs\n",
        "        pars= np.asarray(tf.squeeze(pars)).reshape(1,2)  \n",
        "        sigma , mu = np.hsplit(pars , 2)\n",
        "        sigma = tf.exp(sigma) #get rid of negative sigma\n",
        "        #sigma= abs(sigma)\n",
        "        action_probabilities = tfp.distributions.Normal(mu , sigma) #normal distribution with mu,sigma pars  \n",
        "        #log_prob = action_probabilities.log_prob(action_probabilities) #log (gonna be used for gradient)\n",
        "        action = action_probabilities.sample() #choose action (most likely to be chosen with higher probability)\n",
        "        action = tf.tanh(action) * 0.07 #action: continuous num in range(-0.07, 0.07)((-4,4) degree_\n",
        "        self.action = action  \n",
        "        return action #cast tensor to numpy(openAI gym doesnt take tensor)\n",
        "\n",
        "    def save_models(self):\n",
        "        #print('... saving models ...')\n",
        "        self.actor.save_weights(self.actor.checkpoint_file)\n",
        "        self.critic.save_weights(self.critic.checkpoint_file)\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_weights(self.actor.checkpoint_file)\n",
        "        self.critic.load_weights(self.critic.checkpoint_file)\n",
        "        \n",
        "    def learn(self, state, reward, state_,done):\n",
        "        #print(\"state before \")\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
        "        reward = tf.convert_to_tensor(reward, dtype=tf.float32) # not fed to NN -> no need to reshape\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            state_value = self.critic(state)\n",
        "            state_value_ = self.critic(state_)\n",
        "            state_value = tf.squeeze(state_value) #squeeze Removes dims of size 1 from the shape of a tensor.\n",
        "            state_value_ = tf.squeeze(state_value_)\n",
        "            pars= self.actor(state)\n",
        "            #pars= np.asarray(tf.squeeze(pars)).reshape(1,2)\n",
        "            #mu , sigma= np.hsplit(pars , 2)\n",
        "            #mu = np.squeeze(mu)\n",
        "            #sigma = np.squeeze(sigma)\n",
        "            mu = pars[0,0]\n",
        "            sigma = pars[0,1]\n",
        "            #print(sigma)\n",
        "            #sigma = tf.exp(sigma)\n",
        "            #print(sigma)\n",
        "            action_probs = tfp.distributions.Normal(mu, abs(sigma)) #policy \n",
        "            log_prob = action_probs.log_prob(self.action[0,0] )\n",
        "            #print(mu,sigma)\n",
        "            #print(log_prob)\n",
        "                      \n",
        "            #TD error: \n",
        "            TD= self.gamma*state_value_*(1-int(done)) - state_value \n",
        "            delta = reward + TD #1-done: terminal stRemoves dimensions of size 1 from the shape of a tensor.ate zero effect \n",
        "            actor_loss = (-log_prob*delta)            \n",
        "            critic_loss = (delta**2) \n",
        "            #print(\"sig\", sigma , \"ac\", actor_loss, \"cr\", critic_loss)\n",
        "  \n",
        "            \n",
        "        gradient1 = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        \n",
        "        self.actor.optimizer.apply_gradients((grad , var) for (grad , var) in zip(gradient1, self.actor.trainable_variables) if grad is not None)\n",
        "        #if grad is not None\n",
        "            \n",
        "        gradient2 = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients((grad , var) for (grad , var) in zip(gradient2, self.critic.trainable_variables) if grad is not None)\n",
        "        # if grad is not None\n",
        "        return critic_loss, actor_loss, gradient1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7jAXNqRskpg"
      },
      "outputs": [],
      "source": [
        "from numpy import genfromtxt\n",
        "class lateralenv:\n",
        "    def __init__(self, roadfile, data_length , n_episodes, episode_length):\n",
        "        #constants\n",
        "        dt=0.1\n",
        "        vx=10\n",
        "        iz= 2278.8\n",
        "        m=1300\n",
        "        a1=1; a2=1.5\n",
        "        caf = 60000; car= 60000\n",
        "        cb= -(caf + car); cr= (-a1*caf + a2*car)/ vx\n",
        "        db= -(a1* caf - a2*car); dr= -(a1**2 *caf + a2**2*car) / vx\n",
        "        cd = caf; dd= a1*caf\n",
        "        self.constants=[dt, vx, iz, m, cb, cr, db, dr, cd, dd]\n",
        "\n",
        "        self.data_length = data_length \n",
        "        self.n_episodes = n_episodes \n",
        "        self.episode_length=episode_length\n",
        "        self.episode_length_cnt =episode_length\n",
        "        \n",
        "        #x = np.linspace(0, 10*math.pi, 20000).reshape(20000,1) #0<x<314\n",
        "        x= np.arange(0, self.data_length).reshape(self.data_length, 1) \n",
        "        y= 50*np.sin(x/200)\n",
        "        self.road = geom.LineString(zip(x,y))\n",
        "        \n",
        "        #self.road = genfromtxt(roadfile, delimiter=',')\n",
        "        \n",
        "        heading_angle = [np.arctan2(y[i+1]-y[i] , x[i+1]-x[i]) for i in range(y.shape[0]-1)] #rad #56.3\n",
        "        heading_angle.insert(0,heading_angle[0]) #append last value to adjust the shape          _____EDITED_____        \n",
        "        heading_angle= np.asfarray(heading_angle).reshape(self.data_length,1)\n",
        "\n",
        "        #print('_psiiiiiii', heading_angle[0])\n",
        "\n",
        "        #init vars\n",
        "        self.vy0=0; self.r0=0; self.x0= 3; self.y0= 0; self.psi0= 0.27 #self.psi0= heading_angle[0] \n",
        "        \n",
        "        self.vars0= np.array([[self.vy0, self.r0, self.x0, self.y0, self.psi0]], dtype='float64').T #1,5 vars0\n",
        "        self.vars= self.vars0\n",
        "        self.vars_= np.zeros((5,1))\n",
        "        self.score = 0\n",
        "        self.index=0\n",
        "        self.Done=0\n",
        "        self.coordinates=[]\n",
        "        self.vys = []\n",
        "        self.vymax = -10\n",
        "\n",
        "        point0 = geom.Point(self.x0, self.y0)\n",
        "        dist0, angle_diff0, _ = self.dist_diff(point0, 0, limit= 1) # sefr alaki \n",
        "        future_point0= self.preview(point0)\n",
        "        future_dist0 , future_ang0 , _ = self.dist_diff(future_point0, 0, limit=1) # sefr alaki\n",
        "        self.state0= np.array([dist0, angle_diff0]) #(1,2)\n",
        "        #self.state0= np.array([dist0, angle_diff0, future_dist0, future_ang0]) #(1,4)\n",
        "        #print(\"state0 = \", self.state0)\n",
        "       \n",
        "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "    def dist_diff(self, point,ep, limit):\n",
        "        ##1: based on where the car is supposed to be on this iteration\n",
        "        #self.index = self.index+1\n",
        "        #dist = self.data[self.index, 0:2] - coordinate_ #(1,2)\n",
        "        #angle_diff= self.data[self.index,2] - psi_  \n",
        "        #________________________________________________________________________\n",
        "        \n",
        "        ## 2: based on where the car is supposed to be if the driven distance was along the road\n",
        "        # driving_distance= (vy**2 + vx**2)**0.5 *dt #driving angle : psi_ \n",
        "        # #(dx^2+dy^2)^0.5= distance , dy=1.5dx -> (3.25dx^2)*0.5 = distance -> 1.802 dx = distance\n",
        "        # dx= driving_distance / math.sqrt(3.25); dy= dx*1.5\n",
        "        # dist= ((dx - x_)**2 + (dy- y_)**2)**0.5  \n",
        "        #angle_diff =\n",
        "         #_______________________________________________________________________\n",
        "\n",
        "        #3: based on car's vertical disance with the road          \n",
        "        dist = point.distance(self.road) \n",
        "        dist_z = math.sqrt((point.y-self.y0)**2+(point.x-self.x0)**2)\n",
        "        limited_dist= max(dist, 0.01)\n",
        "        limited_dist= min(limited_dist, 100)\n",
        "\n",
        "        nearestP = nearest_points(self.road, point)[0]\n",
        "        angle_diff= np.arctan2(nearestP.centroid.y, nearestP.centroid.x) - np.arctan2(point.y,point.x) #pos/neg mide        \n",
        "        limited_angle_diff=max(angle_diff, 0.01)\n",
        "        limited_angle_diff=min(limited_angle_diff , 100) #-> max reward = 5000, min reward 5e-5\n",
        "        if (limit==1):\n",
        "          return limited_dist, limited_angle_diff , dist_z\n",
        "        else:\n",
        "          return dist, angle_diff , dist_z\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________________________________________\n",
        "\n",
        "    def preview(self, point):\n",
        "        action = 0\n",
        "        dt, vx, iz, m, cb, cr, db, dr, cd, dd= self.constants\n",
        "        vy, r, x, y, psi = np.vsplit(self.vars,5)\n",
        "        #dt = dt*5 # 5 step forward preview \n",
        "        #calc new state\n",
        "        par_mat1 = np.array([[cb/(m*vx), cr/m-vx,0,0,0],\n",
        "                           [db/(iz*vx), dr/iz, 0,0,0],\n",
        "                           [-math.sin(psi),0,0,0,0],\n",
        "                           [math.cos(psi),0,0,0,0],\n",
        "                           [0,1,0,0,0]])\n",
        "        \n",
        "        par_mat2 = np.array([[cd* action /m],[dd*action/iz], [vx*math.cos(psi)],\n",
        "                    [vx*math.sin(psi)],[0]], dtype='float64') \n",
        "     \n",
        "        var_dot_mat = par_mat1 @ self.vars + par_mat2  #(5,1)= (5,5)@(5,1)+(5,1)\n",
        "        self.vars_= self.vars + dt* var_dot_mat #(5,1) =(5,1)+(5,1)\n",
        "        vy_, r_, x_, y_, psi_= np.vsplit(self.vars_,5)\n",
        "        future_point= geom.Point(x_, y_)\n",
        "\n",
        "        return future_point\n",
        "\n",
        "    def step(self, action,stp_cnt):\n",
        "       \n",
        "        dt, vx, iz, m, cb, cr, db, dr, cd, dd= self.constants\n",
        "        vy, r, x, y, psi = np.vsplit(self.vars,5)\n",
        "        \n",
        "        #calc new state\n",
        "        par_mat1 = np.array([[cb/(m*vx), cr/m-vx,0,0,0],\n",
        "                           [db/(iz*vx), dr/iz, 0,0,0],\n",
        "                           [-math.sin(psi),0,0,0,0],\n",
        "                           [math.cos(psi),0,0,0,0],\n",
        "                           [0,1,0,0,0]])\n",
        "        \n",
        "        par_mat2 = np.array([[cd* action /m],[dd*action/iz], [vx*math.cos(psi)],\n",
        "                    [vx*math.sin(psi)],[0]], dtype='float64') \n",
        "     \n",
        "        var_dot_mat = par_mat1 @ self.vars + par_mat2  #(5,1)= (5,5)@(5,1)+(5,1)\n",
        "        self.vars_= self.vars + dt* var_dot_mat #(5,1) =(5,1)+(5,1)\n",
        "        vy_, r_, x_, y_, psi_= np.vsplit(self.vars_,5)\n",
        "        #print(\"vy , vy_\", self.vars[0], self.vars_[0])\n",
        "        # if self.vars_[0] > self.vymax:\n",
        "        #     self.vymax = self.vars_[0]\n",
        "#        print(\"max vy: \", self.vymax)\n",
        "\n",
        "        point = geom.Point(x_, y_)\n",
        "        #print(\"pppppppppp\", point)\n",
        "        dist, ang_diff, dist_z = lateralenv.dist_diff(self,point,ep, limit=0)\n",
        "        #print(\"diiiiiiiiiif\", dist, ang_diff)\n",
        "        future_point= lateralenv.preview(self, point)\n",
        "        future_dist , future_ang , _ = lateralenv.dist_diff(self, future_point, ep, limit=0)\n",
        "          \n",
        "        '''dist = point.distance(self.road) \n",
        "        limited_dist= max(dist, 0.01)\n",
        "        limited_dist= min(limited_dist, 100)\n",
        "\n",
        "        nearestP = nearest_points(self.road, point)[0]\n",
        "        angle_diff= np.arctan2(nearestP.centroid.y, nearestP.centroid.x) - np.arctan2(y_,x_)[0] #pos/neg mide\n",
        "        limited_angle_diff=max(angle_diff, 0.01)\n",
        "        limited_angle_diff=min(limited_angle_diff , 100) #-> max reward = 5000, min reward 5e-5'''\n",
        "\n",
        "        #action_weight = 10      \n",
        "        #preview_weight = 0.01\n",
        "        #reward = 1/(dist**2 + ang_diff**2) # + preview_weight/(future_dist**2 + future_ang**2) #+ action * action_weight\n",
        "        reward = 1/(dist + ang_diff)\n",
        "        #print(\"dist\", dist, \"ang\", ang_diff, \"reward\", reward)\n",
        "        #reward = 5\n",
        "\n",
        "        # if dist > 2:\n",
        "        #     makhraj = -1/dist\n",
        "        # #try: \n",
        "        # else:\n",
        "        #     min = 1e-1\n",
        "        #     max = 1e+1\n",
        "        #     makhraj= abs(dist)**2 if abs(dist)**2 > min else min #max reward= 10\n",
        "            \n",
        "        '''except OverflowError as err:\n",
        "            makhraj = max #min reward = 0.1'''\n",
        "\n",
        "        #reward = dist_z/(stp_cnt+1)\n",
        "        reward = - dist_z/10\n",
        "\n",
        "        #for next step\n",
        "        self.coordinates.append(self.vars[2:4,0])\n",
        "        self.vys.append(self.vars[0])\n",
        "        self.vars = self.vars_ \n",
        "        #self.state_ = np.hstack((dist, ang_diff, future_dist, future_ang)) #real state (not limited)\n",
        "        self.state_ = np.hstack((dist, ang_diff)) #real state (not limited)\n",
        "\n",
        "        #self.futurestate_ = np.hstack((future_dist, future_ang)) #real state (not limited)\n",
        " \n",
        "        self.episode_length_cnt = self.episode_length_cnt -1\n",
        "        \n",
        "        if self.episode_length_cnt==0: \n",
        "            self.Done = 1\n",
        "\n",
        "        return  self.state_, reward, self.Done,  #state:(dist, ang_dif)\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def render(self, ep, score):\n",
        "        #plt.subplot(1,2,2)\n",
        "        #worksheet.write(ep+1, 0, int(aloss))\n",
        "        #worksheet.write(ep+1, 1, int(closs))\n",
        "        #worksheet.write(ep+1, 2, score) \n",
        "        \n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        \n",
        "        plt.plot(self.road.coords.xy[0][0:int(self.episode_length)*3], self.road.coords.xy[1][0:int(self.episode_length)*3],'r') #road \n",
        "        plt.plot(np.array(self.coordinates)[:,0], np.array(self.coordinates)[:,1], label =score) #path\n",
        "        plt.legend()\n",
        "        if (ep%10 == 0):\n",
        "          # plt.xlabel(\"x\")\n",
        "          # plt.ylabel(\"y\")\n",
        "          # plt.plot(self.data[0:self.episode_length*2, 0], self.data[0:self.episode_length*2,1],'r') #road \n",
        "          # plt.plot(np.array(self.coordinates)[:,0], np.array(self.coordinates)[:,1], label =score) #path\n",
        "          # plt.legend()\n",
        "          plt.savefig(f\"drive/MyDrive/RL paper/path{ep}.jpg\")\n",
        "          plt.cla() \n",
        "          #if (ep%50 ==0):\n",
        "                \n",
        "        pass\n",
        "    \n",
        "    def reset(self): # before each episode \n",
        "        self.Done= 0\n",
        "        self.episode_length_cnt = self.episode_length \n",
        "        \n",
        "        p = geom.Point(self.vars_[2,0], self.vars_[3,0])\n",
        "        #init_coord = nearest_points(self.road, p)[0]\n",
        "        #self.state = np.hstack((init_coord, math.atan(1.5)))\n",
        "        #self.state = np.array([[0,0,math.atan(1.5)]]) #shape(1,3)\n",
        "        \n",
        "        self.vars= self.vars0\n",
        "        self.coordinates = []\n",
        "        return self.state0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "fw_1Aum5snkq",
        "outputId": "7279175d-0642-41da-bf7a-8c0c7ea2fb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  1 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  2 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  3 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  4 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  5 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  6 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  7 score -12524.999999999976 avg_score -12524.999999999975 reward -49.999999999999694\n",
            "episode  8 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  9 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  10 score -12524.999999999976 avg_score -12524.999999999975 reward -49.999999999999694\n",
            "episode  11 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n",
            "episode  12 score -12524.999999999976 avg_score -12524.999999999976 reward -49.999999999999694\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a4c5e99a53c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mstates_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m#print(\"action\", action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-4859847983f6>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get rid of negative sigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#sigma= abs(sigma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0maction_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#normal distribution with mu,sigma pars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m#log_prob = action_probabilities.log_prob(action_probabilities) #log (gonna be used for gradient)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_probabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#choose action (most likely to be chosen with higher probability)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-326>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args, allow_nan_stats, name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36mwrapped_init\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0;31m# called, here is the place to do it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0mself_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m       \u001b[0mdefault_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m       \u001b[0;31m# Note: if we ever want to override things set in `self` by subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m       \u001b[0;31m# `__init__`, here is the place to do it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m    144\u001b[0m           \u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m           \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dtype, reparameterization_type, validate_args, allow_nan_stats, parameters, graph_parents, name)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters_sanitized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_parents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_parents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     self._defer_all_assertions = (\n\u001b[1;32m    632\u001b[0m         auto_composite_tensor.is_deferred_assertion_context())\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/autotrackable.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_self_setattr_tracking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       value = data_structures.sticky_attribute_assignment(\n\u001b[0;32m---> 68\u001b[0;31m           trackable=self, value=value, name=name)\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36msticky_attribute_assignment\u001b[0;34m(trackable, name, value)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# name, since assigning a new variable to an attribute has\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# historically been fine (e.g. Adam did this).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         overwrite=True)\n\u001b[0m\u001b[1;32m    165\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_track_trackable\u001b[0;34m(self, trackable, name, overwrite)\u001b[0m\n\u001b[1;32m   1016\u001b[0m           self._self_unconditional_checkpoint_dependencies[\n\u001b[1;32m   1017\u001b[0m               index] = new_reference\n\u001b[0;32m-> 1018\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mcurrent_object\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_unconditional_checkpoint_dependencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_reference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_deferred_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUVfrH8c+TBIh0EJASqgaFUAIEEAuroCisZVl1NauIFfvPuiIiin3tXewLuIoFZEXECoKiUkJv0kF67xBS5vn9cYYQSIAEMjkzk+f9es0rM/dO+eYmM8/ce849R1QVY4wxJrcY3wGMMcaEHysOxhhj8rDiYIwxJg8rDsYYY/Kw4mCMMSaPON8BikK1atW0QYMGvmMYY0xEmTJlykZVrZ7fuqgoDg0aNCAtLc13DGOMiSgisvxQ6+ywkjHGmDysOBhjjMnDioMxxpg8oqLNwZhokZmZycqVK0lPT/cdxUSR+Ph4EhISKFWqVIEfY8XBmDCycuVKKlSoQIMGDRAR33FMFFBVNm3axMqVK2nYsGGBH2eHlYwJI+np6Rx//PFWGEyRERGOP/74Qu+NWnEwJsxYYTBF7Wj+p6w4mLDw+8AHWDB1nO8YxpggKw7Guxk/fU6HZQPYNHmo7ygmlz/++IMOHTpQpkwZnn/++ZzlK1as4Oyzz6Zp06YkJSXxyiuv5Kzr378/derUITk5meTkZEaNGgXADz/8QJs2bWjevDlt2rRhzJgxeV7voosuolmzZvlm2bJlC927d6dFixa0a9eO2bNn56x75ZVXaNasGUlJSbz88ss5y2fMmEGHDh1o3rw5F154Idu3bwcgIyODa6+9lubNm9OyZUvGjh2b85hPP/2UFi1akJSURO/evXOWL1++nM6dO9OiRQvOOussVq5cmbOud+/eNGvWjGbNmvHpp5/mLB8zZgytW7emWbNm9OzZk6ysrKP+XS6//PKcbdqgQQOSk5Nz1s2cOZMOHTqQlJRE8+bNi64zg6pG/KVNmzZqItPWjWt1/SP1demjzTR9zy7fcbybO3eu7wg51q1bp5MmTdIHH3xQn3vuuZzlq1ev1ilTpqiq6vbt2zUxMVHnzJmjqqqPPPLIAffdZ+rUqbpq1SpVVZ01a5bWrl37gPXDhg3T1NRUTUpKyjfLfffdp/3791dV1Xnz5mmnTp1ynispKUl37dqlmZmZ2rlzZ124cKGqqqakpOjYsWNVVfX999/Xhx56SFVVX3/9db3mmmtyfsfWrVtrdna2bty4UevWravr169XVdWrr75af/zxR1VVvfTSS3XgwIGqqjp69Gi96qqrVFV15MiRes4552hmZqbu3LlTU1JSdNu2bZqdna0JCQk6f/58VVXt16+fvvfee0f9u+R2zz336KOPPqqqqpmZmdq8eXOdPn26qqpu3LhRs7Ky8t2G+f1vAWl6iM9V23MwXi0cdCuVdTtZFw+gTHxZ33FMLjVq1KBt27Z5uj/WqlWL1q1bA1ChQgWaNGnCqlWrDvtcrVq1onbt2gAkJSWxZ88e9u7dC8DOnTt58cUXeeihhw75+Llz59KpUycATjnlFJYtW8a6deuYN28e7du3p2zZssTFxfGXv/yFL774AoAFCxbQsWNHAM4991yGDRuW57lq1KhB5cqVSUtLY8mSJSQmJlK9uhtq6Jxzzsn3MWeffTZffvllzvKOHTsSFxdHuXLlaNGiBd9++y2bNm2idOnSNG7c+LCvX9DfZR9V5bPPPiM1NRWA77//nhYtWtCyZUsAjj/+eGJjYw/7tygo68pqvJn67UBStv/I7/VvokPLM3zHCTuPfjWHuau3F+lzNq1dkUcuTCqy51u2bBnTpk2jffv2Octef/11Bg8eTEpKCi+88AJVqlQ54DHDhg2jdevWlClTBoB+/fpx7733Urbsob8ctGzZki+++IIzzzyTSZMmsXz5clauXEmzZs3o27cvmzZt4rjjjmPUqFGkpKQArgh9+eWX/O1vf+Pzzz9nxYoVOc81YsQIUlNTWbFiBVOmTGHFihV06tSJ+fPns2zZMhISEvjf//5HRkbGAa9/5513Mnz4cHbs2MGmTZto2bIljz76KPfeey+7d+/mp59+omnTplSrVo2srCzS0tJISUlh6NChB7x+YX+XfX755RdOOOEEEhMTAVcARYTzzjuPDRs2cMUVV3D//fcfy580h+05GC82rVtJwwn9WBh7EilXPe47jjkKO3fu5JJLLuHll1+mYsWKANxyyy0sXryY6dOnU6tWLe69994DHjNnzhx69+7N22+/DcD06dNZvHgx3bt3P+xrPfDAA2zdupXk5GRee+01WrVqRWxsLE2aNKF379506dKF888/n+Tk5Jxvzh988AFvvvkmbdq0YceOHZQuXRqA6667joSEBFJSUrjrrrs47bTTiI2NpUqVKgwYMIDLL7+cM888kwYNGuQ81/PPP8+4ceNo1aoV48aNo06dOsTGxtKlSxe6devGaaedRmpqKh06dCA2NhYR4ZNPPuHuu++mXbt2VKhQIee5juZ32WfIkCE5ew0AWVlZjB8/no8++ojx48czfPhwRo8efbR/0gMd6nhTJF2szSGyBLKzdeozXTX94eN16dzJvuOEFd9tDq+//rq2bNlSW7ZsmdNGkF87QkZGhnbp0kVfeOGFQz7X0qVLD2hDWLFihSYmJur48eNzlr355ptaq1YtrV+/vtapU0dLlSqlf/nLXw6bMRAIaP369XXbtm151vXp00ffeOONPMvnz5+vbdu2zff5OnTokNNmktvbb7+t//rXv/Is37Fjh9apUyff50pNTdWvv/46z/LvvvtOL7vssmP6XTIzM7VGjRq6YsWKnGVDhgzRq6++Ouf2Y489ps8++2y+2Qrb5hDyD27gA2A9MDvXsueAP4CZwHCgcnB5A2APMD14easgr2HFIbJM/vJN1Ucq6u8fPuw7StjxXRzyc3BxCAQC2qNHD73zzjvz3Hf16tU511988UW9/PLLVVV1y5Yt2qJFCx02bNghX+fgYpLbli1bdO/evaqq+s4772iPHj1y1q1bt05VVZcvX64nn3yybtmy5YDl2dnZ2qNHD33//fdVVXXXrl26c+dOVVX9/vvv9cwzz8zzXJs3b9aWLVvmNChv2LBBs7OzVVX1wQcf1H79+qmqalZWlm7cuFFVVWfMmKFJSUmamZl5wHOlp6drp06ddPTo0Uf9u6iqfvPNN9qxY8cDtsvmzZu1VatWBzRijxw5Mt9tGI7FoSPQ+qDi0AWIC15/BnhG9xeH2YV9DSsOkWPtikW67ZGaOveJUzUr+CYy+4VTcVizZo3WqVNHK1SooJUqVdI6derotm3b9JdfflFAmzdvnrOXse/b8lVXXaXNmjXT5s2b64UXXphTLB5//HEtW7Zszv1btmyZ80G4z8HFYcCAATpgwABVVf3tt980MTFRGzdurN27d9fNmzfn3O+MM87QJk2aaIsWLXJ6F6mqvvzyy5qYmKiJiYnau3dvDQQCOa/TuHFjPeWUU7Rz5866bNmynMdcccUV2qRJE23SpIkOGTIkZ/nnn3+uJ510kiYmJur111+v6enpqqq6Z8+enPu3b99ep02blvOY++67T0855RRt3LixvvTSSznLj+Z3UVXt2bNnzvbI7cMPP9SmTZtqUlJSvns6+xS2OIhbH1oi0gAYqap5OjGLSHfgUlW98nD3O5yUlBS1yX7CnwYCzHr2XE7aM4vNPcaQcFKh/swlwrx582jSpInvGCYK5fe/JSJTVDUlv/uHQ4P0dcA3uW43FJFpIjJORM481INEpJeIpIlI2oYNG0Kf0hyzScNeokV6GrOa3muFwZgw57U4iEhfIAv4KLhoDVBPVVsB9wAfi0jF/B6rqu+oaoqqpuzrl2zC16ol82g++xlmlWlF20vv8x3HGHME3oqDiFwDXABcGTz2haruVdVNwetTgMVAY18ZTdEIZGez7ZMbySaG6le+S0wRnaRjjAkdL8VBRM4H7gcuUtXduZZXF5HY4PVGQCKwxEdGU3QmffoUTTNmMS+5LzXrJfqOY4wpgJCfIS0iQ4CzgGoishJ4BOgDlAF+CA4lO0FVb8b1bHpMRDKBAHCzqm4OdUYTOsvnTyd5/itML9eBthff5juOMaaAQl4cVDU1n8XvH+K+w4BhoU1kiktWZgZ7P7+RdClDwtXvIDHh0P/BGFMQ9m41IZP2UX8aZy1gcdtHqVaznu84ppBsyG4bsjviL3YSXPhZNPN33ftwFU177iLfUSJKOJ0EZ0N225DdxhSpjL3pyPCb2C4VaNTzLd9xzFGyIbttyG5jitSUwQ/QIbCM6acPILl6Ld9xItc3D8DaWUX7nDWbQ9d/F9nT2ZDdNmS3MQUyP20M7VYOZHLlriSf+0/fcUwI2ZDdNmR32F+szSE87Nm1Q5c/2kTXPNJIt23Z6DtORPLd5mBDdu9nQ3aHwYf7sV6sOISH39+8SfWRijpz3P98R4lYvotDfmzIbhuyO2IvVhz8m/PbKM1+uJJOeO0a31EiWjgVBxuy24bsLprjUx7ZkN1+7dqxlW0vtiOAUOWeiZSrUNl3pIhlQ3abUInEIbtNhJs98E5qBtaz4/xXrTAYEyWsOJhjMnPsMNpv+h+Tav2TJu3P8x3HGFNErDiYo7Zty0Zqjr2P5TF1Se75nO84USMaDvWa8HI0/1NWHMxRWzDwVqrqVjIufJP448r5jhMV4uPj2bRpkxUIU2RUlU2bNhEfH1+ox9kZ0uaoTPv+v7Td9h2/17uBDq06+o4TNRISEli5ciU29a0pSvHx8SQkJBTqMVYcTKFtXr+Ker89yKLYE2lz1ZO+40SVUqVK0bBhQ98xjLHDSqZwNBBg2aCbqaC7iP37AEqXKdyuqjEmMlhxMIUyZdR7tN71M1NOvJWGSe2P/ABjTESy4mAKbMPqZSSm9Wd+3Cm0++cjvuMYY0KoWIqDiHwgIutFZHauZVVF5AcRWRj8WSW4XETkVRFZJCIzRaR1cWQ0h6eBAKs/vJHSmknZy98hNs6aq4yJZsW15zAQOP+gZQ8Ao1U1ERgdvA3QFUgMXnoBA4opozmMycNfpeWeScw45W7qJrb0HccYE2LFUhxU9Wdg80GLLwYGBa8PAv6Wa/ng4LhQE4DKImIzxni0etl8kmY+zZzSLWn3j95HfoAxJuL5bHM4QVXXBK+vBU4IXq8DrMh1v5XBZQcQkV4ikiYiadYnPHQC2dls+fhGFKHKP98lpoimIDTGhLewaJAODh1bqFNCVfUdVU1R1ZR9c76aojfps2dIypjB3BYPULvByb7jGGOKic/isG7f4aLgz/XB5auAurnulxBcZorZioUzaPnHS8w4rh1tu/+f7zjGmGLksziMAHoGr/cEvsy1/Opgr6VTgW25Dj+ZYpKdlcXuT3uRIaWo3eNdJCYsdjKNMcWkuLqyDgF+B04WkZUicj3wb+BcEVkInBO8DTAKWAIsAt4Fbi2OjOZAkz5+lJOz/mBhSn+q127gO44xppgVS2d1VU09xKrO+dxXgdtCm8gcztI5E2mz+E2mVuhIm243+I5jjPHAjhWYA2TsTSf7i1vYIeVocPVbdjjJmBLK3vnmAFP+25eTshfz52lPUbVGnh7ExpgSwoqDybFw2s+0/fMDJlfqQqsuV/mOY4zxyIqDASB9zy5Kf3Urm6UyjXu+6TuOMcYzKw4GgOmD/kX9wArWnvU8laraSYXGlHRWHAzzJn5HuzUfM/H4i2lx1iW+4xhjwoAVhxJu146tVPz2DtbG1CCp5yu+4xhjwoQVhxJu9qC7qRVYz5ZzX6Z8xSq+4xhjwoQVhxJs1s/Dab/xCybVvJyk07r5jmOMCSNWHEqo7Vs3UWPMvfwZU4fkni/4jmOMCTNWHEqo+QNvo5puJv2CN4kvW953HGNMmLHiUAJN/+Fj2m79hkkJ19C49Vm+4xhjwpAVhxJm68a1JPzahyUxDWhz9b+P/ABjTIlkxaGEWTzoZirqDrT725QuE+87jjEmTFlxKEGmfP0ebXb8xNSGN3Ni81N9xzHGhDErDiXExrV/cuLkR1gQ15iUK/v7jmOMCXNWHEoADQRYOehG4nUvZS57l7hSpX1HMsaEOSsOJcDkL98gec8Epp98J/VPTvYdxxgTAYplmtD8iMjJwKe5FjUCHgYqAzcCG4LLH1TVUcUcL2qs/XMhTaY/yZwyzWl3+YO+4xhjIoS34qCq84FkABGJBVYBw4FrgZdU9Xlf2aJFIDubDR/dSEUCVEl9j5jYWN+RjDERIlwOK3UGFqvqct9Bosnkoc/TfO80ZjXrTe2Gp/iOY4yJIOFSHK4AhuS6fbuIzBSRD0Qk36FCRaSXiKSJSNqGDRvyu0uJtnLRbJrPfYGZ8Sm0u+Ru33GMMRHGe3EQkdLARcDnwUUDgBNxh5zWAPmOCqeq76hqiqqmVK9uM5fllp2VxY5PbyRL4qjZ4z0kxvuf2RgTYcLhU6MrMFVV1wGo6jpVzVbVAPAu0M5rugg0+ZPHaZI5lwWt+1GjTkPfcYwxESgcikMquQ4piUitXOu6A7OLPVEEWzYvjdYLX2dauTNoc8FNvuMYYyKUt95KACJSDjgXyP0p9qyIJAMKLDtonTmMzIy9ZA69iV1SlnpXv22Hk4wxR81rcVDVXcDxBy3r4SlOxEv7bz86ZC9iWodXaXVCgu84xpgIZl8to8SiGeNJWf4eaRXPodV5PX3HMcZEOCsOUWBv+m7ivryFLVKJxGve8h3HGBMFrDhEgamDetMg8Cdr/vIslapat15jzLGz4hDh/pj0A+1Wf8ikKhfQ8uzLfMcxxkQJKw4RbPfObZT/5g7WSzWaXPOa7zjGmChixSGCzRp0Dwm6hk3nvkSFSlV9xzHGRBErDhFq9vgRtN8wlAnVL6PZ6Rf6jmOMiTJWHCLQjm2bqfbjPayQ2rS85iXfcYwxUciKQwSaN/B2qutGdnV7jePKVfAdxxgThaw4RJgZYz6h3ZavmVSnB6e0Pcd3HGNMlLLiEEG2blxLnZ97szSmPq2vfsZ3HGNMFLPiEEEWDbqVSrqD7IsHUCa+rO84xpgoZsUhQkwZ9R9SdowmrcGNnNTydN9xjDFRzopDBNi4dgWNJvVjYVwiKVc+5juOMaYEsOIQ5jQQYMXgXpTVdEpf8jalSpfxHckYUwJYcQhzaSMG0Gr3b0xLvJ36Tdr4jmOMKSGsOISxdSsXc/L0J5hXKom2VzzkO44xpgSx4hCmNBBg3X9vJE6zqZj6LrFxXiftM8aUMN6Lg4gsE5FZIjJdRNKCy6qKyA8isjD4s4rvnMVt0rAXaZE+hVlJ91GnUZLvOMaYEsZ7cQg6W1WTVTUlePsBYLSqJgKjg7dLjFVL5tF89rPMKtOadpfe5zuOMaYECtdjFRcDZwWvDwLGAr19hSlOgexstg25gYrEUP2qd5GYIqjfK1fCiBHw22/wxx+waRPs2AEZGW69CMTHQ5UqUKMGJCfDOedAly5uuTGmxBFV9RtAZCmwBVDgbVV9R0S2qmrl4HoBtuy7netxvYBeAPXq1WuzfPnyYk4eGhM+eoxTF77A5OQnafu324/uSbKy4K23YPBgmDsXdu06cH1cHJQpA6VLu9uqsHevuwQCB963Rg047TS44w7o1Ono8hhjwpKITMl1xObAdWFQHOqo6ioRqQH8ANwBjMhdDERki6oest0hJSVF09LSiiFtaC3/Yyo1h3Rhbrm2JN/3deH3GkaOhH79YObM/R/yFStCy5bug/2cc+DUU11xOJSdO+Hbb+HHH2H8eFiwADIz3bpy5dzexAsvQMOGR/dLGmPCxuGKg/c2B1VdFfy5HhgOtAPWiUgtgODP9f4SFo+szAz2Dr2J3RJP3avfLnhhCASgb1+oWhUuvBCmT4eaNd03/VWrYNs2+Pln6N8fzjjj8IUBoHx5uPRSt+cxe7Y79PTLL/DXv7rXGj4cGjWCZs3gm2+O+fc2xhyD116Dt98OyVN7LQ4iUk5EKuy7DnQBZgMjgJ7Bu/UEvvSTsPhM/uhhGmctYEm7x6lWs96RHxAIuL2E8uXhqadg+3b3rX7uXFcUXn0VatcumnBnnOH2Snbvhk8+gcREmDMHunVzexBjxhTN6xhjCmbAAPeF8P/+z30OhIDvPYcTgPEiMgOYBHytqt8C/wbOFZGFwDnB21Fr8czfaLP0HaZU6ESbbtce+QEffQSVKsETT7hDPjfc4A4HffcdNGkS2rCXX+4ONc2eDe3bw7Jl0Lmz25NYvDi0r21MSTd+PNSpA7fe6o4KXHyx+0IYAt7bHIpCJLc57E3fzapnO1AxsJW42ydSuVrNQ9954UL429/cP4MI9OjhvkGU9Th89/TpLsfs2S7Tdde5Q1J20p4xRWfzZvfe/+UXd/vcc+Hjj6FatWN62rBucyjppn7Yh0aBZaw885nDF4YHH4STT3aFITkZli6FQYP8FgZwWWbNgk8/dYe43n8fjj8exo71m8uYaPH++1CrlisM9evDpEnw/ffHXBiOxIqDR/PTxtBu5SAmVe5Gcucr8r/T8uVw4onw9NPunINPP4Vp09w/STj5xz/ct5uePV37x9lnw7XX5u0aa4wpmO3boUMHd9g4O9t9BixbBm3bFsvLH7E4iMgdJXH4ilDbs2sHZb++jQ1SjSbXvpH/nQYPdoVhyRLXKLx+vfsQDldxcTBwoDvZrnJldz0hwRU4Y0zBjR/vOpRMmOCOGCxZAg8U70ARBdlzOAGYLCKficj5wZPSzDGaMehe6upqNnZ+gQqVqh64MhCA1FT3LRzg3XfdLmX58sUf9Gh06AAbNsAFF8CaNXDSSTB0qO9UxkSGfv2gY0d38ur997tRDeoVoAdjETticVDVh4BE4H3gGmChiDwlIieGOFvUmvPr15y6/lMmVvs7zc68+MCVmze7vYVPPoHq1WHePLdbGWni4uCrr1zjdCAAl10Gt9ziO5Ux4Ssryx0heOIJdwh59Gh45hlvcQrU5qCuS9Pa4CULqAIMFZFnQ5gtKu3cvoUqP97FSqlJ82tePnDl7NnuG8KyZe6M5tWr3TkFkeymm1wbSaVKrlC0b+/eBMaY/TZuhAYN4NdfoXFj9973PFxNQdoc7hSRKcCzwK9Ac1W9BWgDXBLifFFnzsD/o2ZgAzvPf42y5SvtX/Hll9CqlduV/Ne/3LeGaOkO2qIFrF0LTZu6nhYNG8LWrb5TGRMepk93hWHVKrjoIne0oHLlIz4s1Aqy51AV+Luqnqeqn6tqJoCqBoALQpouysz8aSjtN49gUq1/ckr7LvtXvPCC68McCLhG3GejcIcsPt51ef3rX90osfXqwfz5vlMZ49eoUZCS4r4UPvig+5JYFCMxF4GCtDk8oqr5djdR1XlFHyk6bdu8gZrj/sWymLok93xu/4oHH4T77nOjpI4fv78ROhrFxLhhOO6+2w0Z3qIFTJ7sO5Uxfvz3v67Thqo7oe3JJ30nOkB4lKgSYOHAW6iqW8m8aADxx5VzC2+91fVdLlfOjVXUoYPfkMXlxRfdgGEZGW44cDthzpQ0r73mRhaIiXGjIKem+k6UhxWHYjDtu0GkbP+ByfWuJzH5TLcwNdUNfVG5shur6MQS1vnr9tvdIbTsbDc206hRvhMZUzwefdQNmFe6NPz+uxsKIwxZcQixTetWUv/3h1gUeyIpPYK7jZdd5rqq1qjhBqsrqtFTI03PnvD55+76BRfAsGF+8xgTao8+6obPL1vWzbtSTGc7Hw0rDiGkgQDLB99Med1N7CVvU6p0GbjySndCWM2abnykqlWP/ETR7JJL3F6DiCuatgdhotWTT7rCcNxxboy0k0/2neiwrDiE0JSR79B61y9MPfFWGjZt68Ya+vhjt8cwf77/QfPCxXnnuYZqETdh0ejRvhMZU7Seew4eesgVhlmzwm9stHxYcQiR9auW0njqY/wR14S2/3zEnR08cKAbsXT+fDd9p9mva1e3R6XqisWvv/pOZEzReOUVNwxGfLw7ITRC2hetOISABgKs/fBGSmkm5a54l9i+fd3ZwVWquMbnMDjBJSx17+4mMgoE4Kyz3MlBxkSyIUPgrrtc43NaWtgfSsrNikMITP7iZVqkT2Zmk3uo+81Yd1JbuXJuAK2S3sZwJKmpbqDBrCzXtffPP30nMubojB7t2hhjY905TElJvhMVihWHIrZ66R8kzXqG2WWSaRt30v4ua9OmubYGc2TXX+/mxU5Ph5Yt3bj2xkSSmTPh/PPd9REjwrpX0qF4Kw4iUldEfhKRuSIyR0TuDC7vLyKrRGR68NLNV8bCCmRns2XIDShCjaTbiLn8CneSy9ixkT+AXnHr0wduvtmNwZSUZIP1mcjx55/7B5h8/33oFjEfYQfwueeQBdyrqk2BU4HbRKRpcN1LqpocvERM38ZJnz5NUsYsFtS7iRqX9XCNq0OHlpwzn4vagAH7x2Jq08ZmlTPhb+dOt7ebnu72fq+91neio+atOKjqGlWdGry+A5gH1PGV51j9uWA6yfNfZmaZtrS652U3NMSrr7pGVnP0RoyA1q3dbvqFF/pOY8yhBQJuTvWtW13vxD59fCc6JmHR5iAiDYBWwMTgottFZKaIfHCoKUpFpJeIpIlI2oYNG4opaf6yMjPY81kv9kppGn+0BNm+3fVQuP12r7miQkwMTJzophsdNSri33Amip1/vhvx4Kyz4M03fac5Zt6Lg4iUB4YBd6nqdmAAcCKQDKwBXsjvcar6jqqmqGpK9erViy1vfiZ/3J+Ts+azc15d4ucsdWMFvfSS10xRJS4OZsxwPb7+/W/XPdCYcHL33fDDD+7ktig5idNrcRCRUrjC8JGqfgGgqutUNTs4X8S7QDufGY9k6ZyJtFnyFkt2NKLOZ8GJbL7/3nes6FO1Kvz2m+sWeNVVMHWq70TGOO+9By+/DBUquHNzwmQ+hmPls7eS4OalnqeqL+ZaXivX3boDs4s7W0Fl7E0n8MXN7AnE03DAjKj75wg7LVq44UcCATjzTDe1ojE+jR8PvXq5vduJE6PqBFef81CeDvQAZonIvlNhHwRSRSQZUGAZcJOfeEc29cMHOTV7CZnDMpDMWJgy0YbFCLV//OWfPJoAABWkSURBVMONTfPEE67xb9my6JlO1USWtWvdcNuqbga3Jk18JypS3t5VqjoekHxWRUTX1QVTx5Gy4j/s+iOOcvO2R+U/R9h6/HHXe2nECOjSBcaM8Z3IlDSBgJvec1+X1Qg9l+Fw7PjHUUjfvZMyI28lc08s5b7c7Kb6vOgi37FKluHDoVEj+Okn6NfPdxpT0nTrBqtWuZ9R2oPOisNRmD7oPuoHVnLcsK3Q4aywm/u1RIiJcfNPly3rDjF9843vRKakeOIJ+O47qFcPvvrKd5qQseJQSHMnfEu7tUMITM6A3dVd9zXjR9Wq8OOPbh6Iv/3NnUltTCiNHg0PP+yG354yJao7n0TvbxYCu3Zspeqo29CtCuPUfXO1xlC/OnSAF190Z6S3bWtjMJnQWb3aDecC7kthtWp+84SYFYdCmPvebZwgG+HLPcR89oU7a9f4d9ddbpiStWvdREHGFLVAwH352LsXnnkGzjjDd6KQs+JQQLN+GkbbbSORCXuJvfL+qOydENGGDnUN1GPGuN1+Y4rS+ee7PYeLL4Z//ct3mmJhxaEAtm3ZSP3vb4cN2WRmtrcG6HC0r4H6uONcg+HYsb4TmWjx4ov7h8b44gvfaYqNFYcCWPniZVQotYudv1Sg1PfRMW5KVKpaFb791l3v1s2NjmnMsZg+3e0plC4NEyZEdQP0wUrOb3qUZn76KkmxU8mYAOWH/WwN0OGuY0fo2xf27LF5NMyxSU93I6wGAvD551Czpu9ExcqKw2FsWbeKU6b2R9dmE3PJMzabW6R4/HE49VQ3Z3evXr7TmEjVqRNs2wY33VQiT3K14nAIGgiw6/HOlC6dxaYtZ1Dqplt8RzKF8dNPbhC0d9+FYcN8pzGRpn9/+P13OOUUeOst32m8sOJwCPMf60VCtTVsml6JakMiYrgnk1t8PIwb544Rp6baCXKm4H79FR57zHVu+P1332m8seKQj00/j6bxns/JWCNUenVsiWqEiiotWrhx9jMzXfuDzUFtjmTnTneujCqMHBlVQ3AXln3qHUTT0yk9MBWJUza1eZi4hif6jmSOxR13QNeubs/h0kt9pzHh7owzYNcu10OpUyffabyy4nCQ1T1Pp0K9vSzZ1Jpave7xHccUhZEjXU+T4cPhnXd8pzHh6p573HS0rVrBs8/6TuOdFYdcNvXvTe1GC9m6vhwNX7MB9aJGTIzrox4XB7feCvPn+05kws1337l538uXh59/9p0mLFhxCApMnEClZQNQEfb0GkZMqVK+I5miVL8+DB4M2dlw+uk2QJ/Zb/NmN6qviBvlt3x534nCghUHgN272fXAhcQ1EGYffzW1ku3kqaiUmgpXXgmbNtkAfWa/Dh3cCW+PPQbt2/tOEzbCtjiIyPkiMl9EFonIA6F8rR1dz6DCaXtZvrMuze9+JZQvZXwbPBgaNnQD9D33nO80xrcbb4QFC9ze5EMP+U4TVsKyOIhILPAG0BVoCqSKSNNQvFb2A72pcNJ8MjSO4+7+CrFuq9FtX/tDmTLQuzekpflOZHwZNgzee891V7V5yPMI10/CdsAiVV2iqhnAJ8DFRf4qY8eiv74GdeOY1foRatRpWOQvYcJQjRpuiG9V6NwZdu/2ncgUt9Wr4Z//dF8Wxo1zA+uZA4RrcagDrMh1e2VwWQ4R6SUiaSKStmHDhqN7lUCAXTHlmRHbgdZ/v+Oow5oIdMEFrufS9u1ucDVTcgQCbuytjAw3HHeLFr4ThaVwLQ5HpKrvqGqKqqZUr1796J6kUycqjVtPy37f2uGkkuiNN6BpUzcPRN++vtOY4nLFFbBiBXTpAnfe6TtN2ArXT8RVQN1ctxOCy4wpWr/+6sbQefpp699eEgwa5Ibfrl4dvv7ad5qwFq7FYTKQKCINRaQ0cAUwwnMmE40qV4ZRo1z7Q9euNkFQNFu8GK6/HmJj3ZcCm5vlsMKyOKhqFnA78B0wD/hMVef4TWWi1llnQZ8+rmG6BEwcXyJlZcFpp7mTIN991+ZmKYCwLA4AqjpKVRur6omqapM2m9B66ilISYE5c+C223ynMUXtwgth/Xq45BK49lrfaSJC2BYHY4rduHFQsSK8+aYbrM9Eh9dec3OLJyTAZ5/5ThMxrDgYs0/ZsjB6tBtj59JL3TdNE9lmz4a77oJSpdzEPdYrscBsSxmTW0oK/PvfsHev6wtvEwRFrowM6NjR/Q0//tjtOZgCs+JgzMHuvx/OPhuWLoWrr/adxhytzp1hyxbXxmATPRWaFQdj8vP993D88fDRR+5iIssTT8D48a5X0gcf+E4Tkaw4GJOfuDj45Rd3jPqaa2D5ct+JTEFNnAgPPwzx8fDbb77TRCwrDsYcSpMmrudSVpYb89/aH8Lfzp1wzjnupMb//Q+qVfOdKGJZcTDmcG66CS66CNasge7dfacxR3Lqqa5A3H23Teh0jKw4GHMkw4dDrVowYgS8/bbvNOZQrrvOncTYtq0bbdUcEysOxhxJTIzrIx8X54b5njfPdyJzsMGD4T//gSpVbADFImLFwZiCqF/ffQAFAm5KyfR034nMPvPmub2G2FhXxOPjfSeKClYcjCmo1FQ3queWLa6B2viXnu6KdXY2DBwIJ5/sO1HUsOJgTGG89x4kJ8P06XDDDb7TmDPPdMX6hhvgqqt8p4kqVhyMKazff3fHtt9/312MH3feCWlp0KyZG4bbFCkrDsYUVny8O9EqLg569YKpU30nKnkGD4ZXX4UKFVyxNkXOioMxRyMxET75xDVQ/+UvsH2770QlR1qaGy9pXwN0+fK+E0UlKw7GHK1LLoH77nMnXaWk2BnUxWHjRleMAwFXnJOSfCeKWlYcjDkWzz3nGkUXLnSzjZnQCQSgdWs3neuDD9pIqyHmpTiIyHMi8oeIzBSR4SJSObi8gYjsEZHpwctbPvIZUyhjxkDdujBqFNx7r+800atzZ1ixArp2hSdt5uBQ87Xn8APQTFVbAAuAPrnWLVbV5ODlZj/xjCmEuDjXtbV8eTdsg/VgKnp33gljx0KjRjaFazHxUhxU9XtVzQrenADYFE0mslWtCpMmuUJx4402hENReuUV1zOpYkWYNs2m+iwm4bCVrwO+yXW7oYhME5FxInLmoR4kIr1EJE1E0jZs2BD6lMYcSZMmbnA+gHPPdTPJmWPz5ZduhNXSpV2X4YoVfScqMUJWHETkRxGZnc/l4lz36QtkAfum2loD1FPVVsA9wMciku9/g6q+o6opqppSvXr1UP0axhRO167u0FJGBrRqBVu3+k4UudLSXI8wEdeuc+KJvhOVKHGhemJVPedw60XkGuACoLOqavAxe4G9wetTRGQx0BhIC1VOY4rcXXe53ktvvglNm8KSJTYYXGH9+afrBZad7bqsnn6670Qljq/eSucD9wMXqeruXMuri0hs8HojIBFY4iOjMcfkjTfg7393kwS1aGHnQBTG5s3QsqUbVO/pp+Hyy30nKpF8tTm8DlQAfjioy2pHYKaITAeGAjer6mZPGY05NsOGQceObi/i1FN9p4kMO3e6tputW+GWW+CBB3wnKrFCdljpcFT1pEMsHwYMK+Y4xoTOTz+5UVwnT3btEd98c+THlFQZGe4w3Pr1cOWV7rCc8SYceisZE71iYlwvm/r14dtv3aEmk1dWlhsKY8UKuOAC+O9/fScq8aw4GBNqcXEwdy7Uru3mo7YCcaCsLLd3tWiRGzfpq698JzJYcTCmeJQt69oe9hWISy7xnSg8ZGW5+RjmzIG2bV2XVRMWrDgYU1z2FYhateCLL2wPIiPDTes5fz6ccQZMmGBnP4cR+0sYU5zKloUFC1yBGD4czjqrZHZz3b3bzYmxZIkbUO+XX6wwhBn7axhT3MqXdx+KJ54I48a54+1ZWUd+XLRYu9Y10P/5p+vB9eOPvhOZfFhxMMaH+Hj44w9o0wZmzXKFYudO36lCb84c97tu3Ag9erhhzk1YsuJgjC9xcW78oK5d3bfounVdm0S0Gj3a7SXt3g39+rl5oE3YsuJgjG+jRsHNN7uzgps23T+yazR59lk3Um12Nrz3Hjz2mO9E5gisOBgTDgYMgHffdY3TF18Mjz7qO1HR2Pf79O7tht3+8Ue4/nrfqUwBWHEwJlzccAOMHw/HHQf9+7vunenpvlMdvdWrXfvCiBHu/I5ly6BTJ9+pTAFZcTAmnHTo4NofEhPh11/hhBPcuEyRZvBgaNDAFYTOnWH5cqhZ03cqUwhWHIwJN9WquXMhbr4Ztm+H9u3dHMqRcD5ERgZ06wY9e7q8r77qDiXFeRnj0xwDKw7GhKsBA9xgfWXLug/ZOnVg+nTfqQ7ts8+gShU38mzt2u7M5zvu8J3KHCUrDsaEs/POc+cE/PWv7uSx1q3duEy7dx/5scVl9Wp3OOzyy2HPHvi//3Ojq9q0nhHNioMx4S4+HkaOdHsRlSu7cZmqVIGHH/Z7qCk9Hf75T0hIcOMiJSa68zReecWGwogC9hc0JlLs24vo18/dfvxxqFgR+vYt3uE3tm93bQoVK8KQIVChAnzwgWsnsb2FqGHFwZhIEhPjTiDbssV9a9+7F556yo3X9I9/wNKloXvtyZPdiWxVqrjeSHFx0KePy3LttaF7XeOFl+IgIv1FZFVw/ujpItIt17o+IrJIROaLyHk+8hkT9sqWhY8+gh07XK+mmBj4/HNo1MgNanfnna5L7LGaOtXtJVSrBu3auZ5HlSvDM8+4saCeesoOIUUpUdXif1GR/sBOVX3+oOVNgSFAO6A28CPQWFWzD/d8KSkpmpaWFqK0xkSIIUPg6adh9mzY976uUAEaN4bTT4fTTnM/ExLyPjYQcO0FEya4kWInTnR7IXv2uPWxse7xjz3mhhk3UUFEpqhqSn7rwq3z8cXAJ6q6F1gqIotwheJ3v7GMiQCpqe6SkQH/+Y+7zJ0LU6a4y6uv7r9vTIz7wFd1hSG/hu0qVaBjR7jxRuje3fYQShifxeF2EbkaSAPuVdUtQB1gQq77rAwuy0NEegG9AOrVqxfiqMZEkNKl4aab3AXc4Z+vvnIjwM6bB2vWuD2CPXvcB358PJQrB/XqwSmnuGE7unSxE9dKuJAdVhKRH4H8zpfviysAGwEFHgdqqep1IvI6MEFV/xt8jveBb1R16OFeyw4rGWNM4Xk5rKSq5xTkfiLyLjAyeHMVUDfX6oTgMmOMMcXIV2+lWrludgdmB6+PAK4QkTIi0hBIBCYVdz5jjCnpfB1UfFZEknGHlZYBNwGo6hwR+QyYC2QBtx2pp5Ixxpii56U4qGqPw6x7EniyGOMYY4w5iPVNM8YYk4cVB2OMMXlYcTDGGJOHFQdjjDF5eBlbqaiJyAZg+TE8RTXcSXnhKtzzQfhnDPd8YBmLQrjng/DKWF9Vq+e3IiqKw7ESkbRDnSUYDsI9H4R/xnDPB5axKIR7PoiMjGCHlYwxxuTDioMxxpg8rDg47/gOcAThng/CP2O45wPLWBTCPR9ERkZrczDGGJOX7TkYY4zJw4qDMcaYPEp0cRCR80VkvogsEpEHPGWoKyI/ichcEZkjIncGl1cVkR9EZGHwZ5XgchGRV4OZZ4pI62LMGisi00RkZPB2QxGZGMzyqYiUDi4vE7y9KLi+QTHlqywiQ0XkDxGZJyIdwmk7isjdwb/xbBEZIiLxvrehiHwgIutFZHauZYXeZiLSM3j/hSLSsxgyPhf8O88UkeEiUjnXuj7BjPNF5Lxcy0Pyfs8vX65194qIiki14G0v2/CoqGqJvACxwGKgEVAamAE09ZCjFtA6eL0CsABoCjwLPBBc/gDwTPB6N+AbQIBTgYnFmPUe4GNgZPD2Z8AVwetvAbcEr98KvBW8fgXwaTHlGwTcELxeGqgcLtsRN93tUuC4XNvuGt/bEOgItAZm51pWqG0GVAWWBH9WCV6vEuKMXYC44PVncmVsGnwvlwEaBt/jsaF8v+eXL7i8LvAd7gTdaj634VH9Xj5f3OsvDh2A73Ld7gP0CYNcXwLnAvNx06eCKyDzg9ffBlJz3T/nfiHOlQCMBjrhZu4T3Fme+96gOdsz+IboELweF7yfhDhfpeCHrxy0PCy2I644rAi++eOC2/C8cNiGQIODPngLtc2AVODtXMsPuF8oMh60rjvwUfD6Ae/jfdsx1O/3/PIBQ4GWuDlr9hUHb9uwsJeSfFhp35t1n5XBZd4EDx20AiYCJ6jqmuCqtcAJweu+cr8M3A8EgrePB7aqalY+OXIyBtdvC94/lBoCG4D/BA99vSci5QiT7aiqq4DngT+BNbhtMoXw2ob7FHab+X4vXYf7Ns5hshRrRhG5GFilqjMOWhUW+QqiJBeHsCIi5YFhwF2quj33OnVfJbz1ORaRC4D1qjrFV4YCiMPt2g9Q1VbALtwhkRw+t2PwuP3FuCJWGygHnO8jS2H4/t87EhHpi5s18iPfWfYRkbLAg8DDvrMci5JcHFbhjgnukxBcVuxEpBSuMHykql8EF6+T4FzbwZ/rg8t95D4duEhElgGf4A4tvQJUFpF9swnmzpGTMbi+ErApxBlXAitVdWLw9lBcsQiX7XgOsFRVN6hqJvAFbruG0zbcp7DbzMt7SUSuAS4ArgwWsXDJeCLuS8CM4HsmAZgqIjXDJF+BlOTiMBlIDPYWKY1r9BtR3CFERID3gXmq+mKuVSOAfT0WeuLaIvYtvzrY6+FUYFuuQwAhoap9VDVBVRvgttMYVb0S+Am49BAZ92W/NHj/kH77VNW1wAoROTm4qDNuLvJw2Y5/AqeKSNng33xfvrDZhrkUdpt9B3QRkSrBPaQuwWUhIyLn4w5zXqSquw/KfkWwt1dDIBGYRDG+31V1lqrWUNUGwffMSlynk7WE0TY8Ip8NHr4vuJ4DC3C9GPp6ynAGbrd9JjA9eOmGO748GlgI/AhUDd5fgDeCmWcBKcWc9yz291ZqhHvjLQI+B8oEl8cHby8Krm9UTNmSgbTgtvwfrtdH2GxH4FHgD2A28CGuR43XbQgMwbWBZOI+xK4/mm2GO+6/KHi5thgyLsIdo9/3nnkr1/37BjPOB7rmWh6S93t++Q5av4z9DdJetuHRXGz4DGOMMXmU5MNKxhhjDsGKgzHGmDysOBhjjMnDioMxxpg8rDgYY4zJw4qDMcaYPKw4GGOMycOKgzEhICJtg+P1x4tIOXHzODTzncuYgrKT4IwJERF5Anem83G4cZ+e9hzJmAKz4mBMiATH8JkMpAOnqWq250jGFJgdVjImdI4HyuNm+Iv3nMWYQrE9B2NCRERG4IY4b4ibWe12z5GMKbC4I9/FGFNYInI1kKmqH4tILPCbiHRS1TG+sxlTELbnYIwxJg9rczDGGJOHFQdjjDF5WHEwxhiThxUHY4wxeVhxMMYYk4cVB2OMMXlYcTDGGJPH/wOiMVqmd34REwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "agent = Agent(layer1_dim=128, layer2_dim=64, n_actions=2, alpha_A=0.0003, alpha_C=0.005, gamma=0.99)\n",
        "n_episodes = 100\n",
        "data_length= 50000\n",
        "ep_length = int(data_length/n_episodes)\n",
        "roadfile = 'drive/MyDrive/RL paper/RealRoad1.csv'\n",
        "#env = lateralenv(roadfile, data_length, n_episodes, ep_length)\n",
        "env = lateralenv(roadfile, data_length, n_episodes, ep_length)\n",
        "\n",
        "score_history = []\n",
        "best_score = 0 #reward = 1/positive > 0 -> min score =0    \n",
        "load_checkpoint = False \n",
        "if load_checkpoint:\n",
        "    agent.load_models()\n",
        "\n",
        "#data_path= os.path.join(os.path.abspath(os.getcwd()), \"/Road Samples\")\n",
        "workbook = xlsxwriter.Workbook('drive/MyDrive/RL paper/log.xlsx')\n",
        "log = workbook.add_worksheet(\"ep_per_ep\")\n",
        "log.write(0,0,\"ep / step\")\n",
        "log.write(0,1,\"actor loss\")\n",
        "log.write(0,2,\"critic loss\")\n",
        "log.write(0,3,\"reward\")\n",
        "log.write(0,4,\"distance\")\n",
        "log.write(0,5,\"angle_diff\")\n",
        "\n",
        "#training________________________________________________________________________________________\n",
        "cnt=0\n",
        "for ep in range(1, n_episodes+1): \n",
        "    #file= open(\"/%d.csv\".format(i//4))  \n",
        "    #roadxy = np.loadtxt(file, delimiter=\" \")\n",
        "    #observation = roadxy[roadxy.shape[0]/4*i%4,roadxy.shape[0]//4*(i%4+1)]\n",
        "    #data_chunk = env.data[episode_length * i:episode_length * (i+1) , :]\n",
        "    score=0\n",
        "    al = []; cl=[]; rewards=[]\n",
        "    state = env.reset() #(1,2)\n",
        "    #for j in range(2):\n",
        "    j=0\n",
        "    states_=[]\n",
        "    while not env.Done:\n",
        "        action = agent.choose_action(state)\n",
        "        #print(\"action\", action)\n",
        "        state_, reward, Done = env.step(action,j)\n",
        "        #print(\"reward\", reward)\n",
        "        states_.append(state_)\n",
        "        \n",
        "        #with open('rewards.txt', 'a') as f:\n",
        "            #writer = csv.writer(f)\n",
        "            #writer.writerow(str(reward))\n",
        "            #f.write(str(j)+\" \"+str(reward)+\"\\n\")\n",
        "          #print(j, \"reward :\", reward)\n",
        "        \n",
        "        score = score + reward\n",
        "        rewards.append(reward)\n",
        "\n",
        "        #if not load_checkpoint:\n",
        "        closs, aloss, grad1= agent.learn(state, reward, state_, Done)\n",
        "        #log\n",
        "        log.write((ep-1)*ep_length + j+1, 0, f\"{ep} / {j}\")\n",
        "        log.write((ep-1)*ep_length + j+1, 1, str(closs))\n",
        "        log.write((ep-1)*ep_length + j+1, 2, str(aloss))\n",
        "        log.write((ep-1)*ep_length + j+1, 3, reward)\n",
        "        log.write((ep-1)*ep_length + j+1, 4, state_[0])\n",
        "        log.write((ep-1)*ep_length + j+1, 5, state_[1])\n",
        "        #log.write((ep-1)*ep_length + j+1, 6, int(grad1))\n",
        "\n",
        "        state = state_\n",
        "        j+=1 #step counter\n",
        "        #print(state_)\n",
        "\n",
        "    states_ = np.array(states_)\n",
        "    # m=np.min(states_, axis=0)\n",
        "    # M=np.max(states_, axis=0)\n",
        "    # print(\"episode \", j, \"min\", m, \"max\", M)\n",
        "    \n",
        "    #print(\"max\", states_[0].max())\n",
        "    score_history.append(score)\n",
        "  \n",
        "        #______________ plot score curve)\n",
        "        # ep = [i+1 for i in range(i)]\n",
        "        # x= np.array(ep).reshape(i,1)\n",
        "        # #score_history= np.array(score_history).reshape(i,1)\n",
        "        # plt.xlabel(\"episode\")\n",
        "        # plt.ylabel(\"score\")\n",
        "        # plt.plot(x, np.array(score_history).reshape(i,1))\n",
        "        # plt.savefig(figure_file)\n",
        "\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        if not load_checkpoint:\n",
        "            agent.save_models()\n",
        "    if (ep%1==0):\n",
        "        print('episode ', ep, 'score' ,score, 'avg_score' ,avg_score, 'reward', reward)\n",
        "        env.render(ep, score)\n",
        "   \n",
        "workbook.close()\n",
        "\n",
        "if not load_checkpoint:\n",
        "    ep = [i+1 for i in range(n_episodes)]\n",
        "    x= np.array(ep).reshape(n_episodes,1)\n",
        "    score_history= np.array(score_history).reshape(n_episodes,1)\n",
        "    plt.xlabel(\"episode\")\n",
        "    plt.ylabel(\"score\")\n",
        "    plt.plot(x, score_history)\n",
        "    plt.savefig('scores.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPksc48rTy8s"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd5X-6pIQ8vU"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmPtNVj3Px5c",
        "outputId": "a5c2abac-37c7-43dc-8822-3fe31c3e9d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "308.71529766687416\n",
            "POINT (9 2.2492407018830116)\n"
          ]
        }
      ],
      "source": [
        "# test \n",
        "import numpy as np \n",
        "import math\n",
        "import shapely.geometry as geom\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.ops import nearest_points\n",
        "\n",
        "x= np.arange(0, 10).reshape(10, 1) \n",
        "y= 50*np.sin(x/200)\n",
        "road = geom.LineString(zip(x,y))\n",
        "p= geom.Point(314,50)\n",
        "dist = p.distance(road) \n",
        "print(dist)\n",
        "nearestP = nearest_points(road, p)\n",
        "print(nearestP[0])\n",
        "#angle_diff= np.arctan2(nearestP.centroid.y, nearestP.centroid.x) - self.psi0 #pos/neg mide         \n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlAwco_gsuPK"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "test.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}